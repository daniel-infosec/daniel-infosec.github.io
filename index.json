[{
    "title": "The Pillars of Security Monitoring: A Comprehensive Guide",
    "date": "",
    "description": "Why do the distinctions between threat detection, posture management, compliance monitoring, insider threat, and vulnerability monitoring matter? Discover how understanding these key types of security monitoring can transform your cybersecurity strategy and fortify your defenses against evolving threats.",
    "body": "The Pillars of Security Monitoring: A Comprehensive Guide As you may know by now if you\u0026rsquo;ve read a few of my blog articles, I\u0026rsquo;m a big fan of decomposing ideas in threat detection into various frameworks. This latest blog post will touch on another area that I feel strongly about - specifically, that detection engineers need to take a step back and realize that they\u0026rsquo;re part of a larger \u0026ldquo;security monitoring\u0026rdquo; capability which consists of related but distinct pillars. We\u0026rsquo;ll explore these interrelated pillars of security monitoring: threat detection, posture management, compliance monitoring, abuse detection, insider threat, and vulnerability management. Understanding these differences isn\u0026rsquo;t just academic – it\u0026rsquo;s essential for crafting an effective, nuanced security strategy. Importantly, it will lead into an often opinionated discussion on SOC vs SOC-less models.\nSides of the Same Die Before we break down the differences between these pillars, I feel like I have some responsibility to convince you that they\u0026rsquo;re all part of the same discipline. To that end, let\u0026rsquo;s start with define what I mean when I say \u0026ldquo;security monitoring\u0026rdquo;. Starting with good old Wikipedia, we have a definition for computer security as \u0026ldquo;the protection of computer systems and networks from threats that may result in unauthorized information disclosure, theft of (or damage to) hardware, software, or data, as well as from the disruption or misdirection of the services they provide\u0026rdquo;. Monitoring, per the dictionary, is \u0026ldquo;to watch and check a situation carefully for a period of time in order to discover something about it\u0026rdquo;. It thus follows, then, that security monitoring is the act of protecting computer systems by checking on them in order to reduce systemic risk by identifying to and responding to situations that could result in a loss before the loss (or at least a catastrophic loss) were to occur. Let\u0026rsquo;s thus then see how each of these areas can be interpreted as a version of security monitoring.\n Threat Detection, as I strictly define it for the purpose of this article, is the act of looking for unauthorized access or activity on a computer system. Posture Management is the process of identifying when controls that were applied to a system to make it secure have, for some reason or another, been weakened or removed. Here, our security team is looking for events or identifying configurations that would take a system from a secure state to one that is not. A common example is when an S3 bucket is made public. Compliance Monitoring has its roots in the requirements of governments or other governing bodies. Compliance Monitoring looks for events that violate those requirements. Often there\u0026rsquo;s an overlap with posture management (e.g. the governing body may say that you should monitoring for weakening of security controls). But there can be monitoring unique to compliance such as stating that all production access should be logged and audited. Abuse Detection can vastly overlap with Threat Detection. Abuse is defined as using a system as designed but for purposes that are intended to cause harm. As an example, Threat Detection may look for unauthorized access to an AWS EC2 instance. Whereas Abuse Detection may look to an EC2 instance being used to DDoS a third party. Insider Threat has a strong overlap with Threat Detection. What can differ, as we\u0026rsquo;ll cover in more detail earlier, is where requirements come from and how response needs to be handled. Generally, the initial access to a computer system is authroized, but the user has or develops malicious intents. Some teams will often group Insider Threat as a sub-function of Threat Detection, and I do believe that is a reasonable approach for most organizations. Vulnerability Management is not often grouped with security monitoring by most professionals. However, if we consider what Vulnerability Monitoring consists of, we can see that it matches the definition. With vulnerability monitoring, we\u0026rsquo;re inspecting our systems for weakness (risks) and taking action (patching) before the risk is realized (vulnerability exploited).  Drivers and Sources of Monitoring Requirements Now that we\u0026rsquo;ve aligned that these 6 operations are all sub-categories of security monitoring, we\u0026rsquo;ll dive into the differences starting with where requirements for these pillars originate from.\nThreat Detection Threat detection is driven by a mix of internal strategies and external intel. More often than not, we\u0026rsquo;ve identified some indicator that can be used to determine unauthorized access. Internally, one way that can be accomplished is by analyzing system behavior to determine, for example, an application allowlist. Other sources of indicators can include:\n Threat Intelligence Reports: We\u0026rsquo;re subscribed to various feeds from cybersecurity firms, government agencies, and industry-specific sharing centers. Internal Incident History: We learn from our own past incidents. Security Research: Sometimes our own team or contractors uncover new threats. Emerging Trends: We keep an eye on dark web forums and hacker communities.  For example, if we get a threat intel report about a new malware targeting our industry, a threat detection team will work to develop and implement new detection rules pronto.\nPosture Management Posture management is all about best practices, security frameworks, and risk assessments. We\u0026rsquo;re looking at:\n Security Benchmarks: Organizations like CIS provide guidelines for secure configurations. Cloud Provider Guidelines: The big cloud providers offer their own security best practices. Internal Security Policies: We develop our own standards based on our unique needs. Industry Standards: Frameworks like NIST or ISO 27001 inform our requirements.  So if CIS puts out a new benchmark for AWS configurations, we\u0026rsquo;re likely going to update our monitoring accordingly.\nCompliance Monitoring Compliance monitoring is driven by legal and regulatory requirements. We\u0026rsquo;re talking about:\n Regulatory Bodies: Government agencies or industry regulators setting standards. Legal Requirements: Laws like GDPR, CCPA, or HIPAA. Industry Standards: Things like PCI DSS for the payment card industry. Contractual Obligations: Sometimes our business agreements require certain standards.  For instance, a healthcare provider might need to monitor HIPAA compliance for data access and user authentication. Some government orgs require logging and auditing of all production access.\nAbuse Detection Requirements will be based on the capabilities of our service as well as our terms of acceptable use policies.\n Industry Best Practice: Understand common abuse scenarios like resource hijacking, DoS, hosting inappropriate or malicious content, etc. Customer Reports: While not ideal, if a customer reports active abuse, we should work quickly to both prevent and monitor for it. Internal Risk Assessment and Threat Modeling: During threat models, ensure to account for how public services could be abused.  An undesired, but extremely common, scenario for hosting providers is receiving takedown notices from companies saying that someone is abusing their service to illegal host copyrighted or otherwise protected content.\nInsider Threat Insider threat monitoring is driven by organizational risk management and specific incidents or trends:\n Organizational Risk Assessments: We identify critical assets and potential insider risks. Historical Incidents: Past insider threats inform our strategies. Industry Trends: We learn from insider threats in similar organizations. Regulatory Requirements: Some industries have specific requirements for monitoring insider activities.  For example, a financial institution might need to keep a closer eye on employees with access to sensitive financial data. But we\u0026rsquo;ve got to be careful here – there\u0026rsquo;s a fine line between monitoring and creating bias traps. We need to work closely with HR and legal to avoid issues.\nVulnerability Management Vulnerability management is often reactive to new discoveries and proactive based on regular assessments:\n Vulnerability Databases: We\u0026rsquo;re constantly checking the National Vulnerability Database and CVE list. Vendor Security Advisories: We keep an eye on what software and hardware vendors are saying. Automated Vulnerability Scanners: We run regular scans of our environment. Penetration Testing Results: Findings from our pen tests inform our monitoring. Bug Bounty Programs: We learn from what ethical hackers report.  The Log4j vulnerability is a perfect example of how a new discovery can drive urgent monitoring and patching efforts across the board.\nDifferences in Response Strategy and Ownership As we move into discussions on the different response strategies, there\u0026rsquo;s a few themes I want to focus on. The first is how important false positives and negatives are. Some types of security monitoring are more tolerant of FP and FN whereas for others, there\u0026rsquo;s more wiggle room.\nThreat Detection Threat detection is often seen as the frontline of defense against cyber threats. It focuses on identifying potential security breaches or malicious activities in real-time. Imagine your threat detection tools flagging unusual network traffic from a corporate laptop, indicating the presence of a Remote Access Trojan (RAT). What happens next? The response to a threat detection alert typically resembles a high-stakes fire drill:\n Triage: Quickly assess how severe the threat could be. Is this a false alarm, or do we have a real issue on our hands? Investigation: Dive deeper to figure out the nature and scope of the threat. What system is affected? How did the intruder get in? What is their objective? Containment: Isolate affected systems to prevent the threat from spreading. This might mean taking a server offline or blocking specific network traffic. Eradication: Remove the threat from the environment. This could involve deleting malicious files, disabling compromised accounts, or even rebuilding entire systems. Recovery: Get everything back to normal operations. Restore systems from backups, apply security patches, and confirm that the threat is truly gone.  This process usually involves a dedicated incident response team working around the clock until the threat is neutralized. The goal for incident response is to operate 24/7 with a low mean time to respond (MTTR).\nThe tolerance for false positives and negatives in threat detection is extremely low. False positives can overwhelm the team and lead to alert fatigue, while false negatives can result in catastrophic damage to the company. It’s crucial to strike the right balance and continuously refine detection algorithms to maintain effectiveness.\nPosture Management Posture management shifts the focus from reactive firefighting to proactive defense. It’s all about maintaining the optimal security configuration across your systems and networks. For instance, tools constantly scan your AWS environment, raising alarms if someone accidentally makes a security group too permissive. Here\u0026rsquo;s how the response typically unfolds:\n Notification: Alerts are sent directly to the responsible team (like the cloud infrastructure folks for AWS misconfigurations). Assessment: The team evaluates the alert to determine its validity and decide if immediate action is needed. Is this a minor misconfiguration, or does it expose critical data? Remediation: Apply the necessary fixes to bring the system back into compliance with security policies. This might involve adjusting firewall rules, updating access permissions, or enforcing encryption standards. Verification: Conduct a follow-up scan to ensure the issue has been resolved and no new vulnerabilities have been introduced.  Posture management is often more of a 9-to-5 operation, unless a critical risk is identified that requires urgent attention. Here, the tolerance for false positives can generally be a bit higher since validation is usually quick and often automated. However, false negatives can be disastrous, especially if they involve exposing sensitive data or critical infrastructure to potential attacks.\nCompliance Monitoring Compliance monitoring ensures that your organization adheres to industry standards, government regulations, and internal policies. Let\u0026rsquo;s say an employee installs unauthorized software on their work computer. Here’s how the response might play out:\n Documentation: Log the violation for the inevitable audit. Detailed records are crucial for demonstrating compliance efforts. Notification: Inform the employee\u0026rsquo;s manager about the policy violation. This step helps reinforce the importance of following established procedures. Education: Provide the employee with a refresher on company policies and the importance of compliance. This could involve additional training sessions or workshops. Remediation: Remove the unauthorized software or get it approved through proper channels. Ensure that systems are brought back into compliance with relevant policies. Review: Analyze the incident to determine if updates are needed in your policies or monitoring processes to prevent future violations.  This process often involves collaboration between HR, compliance officers, and management, focusing more on policy enforcement and education rather than technical firefighting.\nFalse positives and negatives can be tricky to balance here. False positives may require additional work with the compliance team to document and understand, while false negatives could lead to regulatory penalties or loss of certifications, impacting the company\u0026rsquo;s ability to operate. It’s crucial to maintain accurate and comprehensive monitoring to minimize these risks.\nAbuse Detection Responding to abuse detection is a bit different from the other pillars because it often involves external parties and requires a fast response to prevent further harm. Here\u0026rsquo;s how you might handle it:\n Immediate Assessment: As soon as an abuse case is identified, assess the situation to understand the scope and impact. This could mean figuring out if a DoS attack is underway or if someone\u0026rsquo;s using your resources to mine cryptocurrency. Containment and Mitigation: Quickly isolate the problem to prevent it from affecting more users or resources. This might involve temporarily suspending the offending account or cutting off network access to the abused resource. Notification and Communication: Inform affected parties, whether they are customers, partners, or internal stakeholders, about what’s happening. Transparency is key to maintaining trust, especially if third parties are involved. Remediation and Prevention: Remove any abusive elements, like malicious scripts or unauthorized access points. Then, work on fortifying your systems to prevent similar incidents in the future. This might involve tightening up acceptable use policies or enhancing your monitoring systems. Legal and Compliance Coordination: Collaborate with your legal and compliance teams to ensure your response is in line with regulatory requirements. In severe cases, legal action against the perpetrators might be necessary.  False positives for abuse detection can have a similar impact to the team threat detection false positives where sigificant churn and wasted time can occur. False negatives can range in impact from increased spend (say an attacker figu)\nInsider Threat Insider threats involve detecting and preventing malicious activities from within your own organization – employees, contractors, or anyone with internal access. Responding to insider threats is a delicate process, as it involves balancing security needs with employee privacy:\n Discreet Investigation: Gather evidence without tipping off the suspect. This might involve monitoring communication channels, reviewing access logs, or analyzing recent activities. Risk Assessment: Determine the potential impact of the insider’s actions. Are they stealing sensitive data? Are they trying to sabotage operations? Legal Consultation: Ensure compliance with labor laws and company policies. Involving legal advisors early on helps avoid potential legal pitfalls. Intervention: Depending on the severity, this could range from a conversation with the employee to involving law enforcement. Handle the situation with care to avoid unnecessary escalation. Mitigation: Implement measures to prevent similar incidents in the future. This could involve updating access controls, improving employee training, or enhancing monitoring capabilities.  This process typically requires close collaboration between HR, legal, and security teams, often with oversight from senior management due to its sensitive nature. It’s essential to maintain a careful balance between security measures and employee trust.\nVulnerability Management Vulnerability management is about staying ahead of potential threats by continuously scanning for and assessing security weaknesses. Think back to the Log4j vulnerability from late 2021 – a prime example of why proactive vulnerability management matters. Here\u0026rsquo;s how the response typically goes:\n Assessment: Identify which systems are affected by the vulnerability and evaluate the potential impact. Prioritize based on the severity and exploitability of the vulnerability. Prioritization: Rank vulnerabilities according to their risk level. Focus on addressing the most critical vulnerabilities first to reduce potential attack surfaces. Patching: Apply the necessary updates or patches to affected systems. This step is crucial for mitigating the risk posed by the vulnerability. Verification: Confirm that patches have been successfully applied and that systems are no longer vulnerable to known exploits. Monitoring: Continuously watch for any attempts to exploit the vulnerability. This ongoing vigilance helps ensure that your defenses remain effective.  Vulnerability management often involves collaboration between security teams and system administrators, with timelines varying based on the severity of the vulnerability. Critical vulnerabilities like Log4j may require an all-hands-on-deck response, while less severe issues can be addressed during routine maintenance windows.\nAdditional Key Differences Tooling and Technology Each pillar has its own tech stack:\n Threat Detection: Usually involves SIEM systems, EDR/XDR tools, network traffic analyzers. Posture Management: Cloud security posture management (CSPM) tools, configuration management databases. Compliance Monitoring: GRC platforms, policy management tools. Vulnerability Management: Vulnerability scanners, patch management systems. Insider Threat: User and Entity Behavior Analytics (UEBA) tools, data loss prevention (DLP) solutions.  Skill Sets and Expertise The people involved in each type of monitoring often have different backgrounds:\n Threat Detection: Incident response specialists, SOC analysts. Posture Management: Cloud security experts, system administrators. Compliance Monitoring: Compliance officers, auditors, legal experts. Vulnerability Management: Security researchers, system administrators. Insider Threat: Behavioral analysts, HR professionals, forensic investigators.  Frequency and Timing The rhythm of monitoring activities varies:\n Threat Detection: It\u0026rsquo;s a 24/7 operation. Posture Management: Regular scans, often daily or weekly. Compliance Monitoring: Periodic audits and ongoing checks. Vulnerability Management: Regular scans and ad-hoc checks for critical vulnerabilities. Insider Threat: Ongoing behavioral analysis with periodic reviews. May be triggered by high-risk events (off-boarding).  Metrics and KPIs We measure success differently across the pillars:\n Threat Detection: We\u0026rsquo;re looking at mean time to detect (MTTD) and mean time to respond (MTTR). Posture Management: Percentage of systems compliant with security baselines. Compliance Monitoring: Audit pass rates, number of policy violations. Vulnerability Management: How quickly we remediate vulnerabilities, patch coverage. Insider Threat: Number of insider incidents detected, time to detect insider threats.  Integration with Business Processes Each pillar fits into the broader business in its own way:\n Threat Detection: Closely tied to incident response and business continuity planning. Posture Management: Integrated with change management and system deployment processes. Compliance Monitoring: Aligned with regulatory reporting and audit processes. Vulnerability Management: Integrated with IT operations and software development lifecycles. Insider Threat: Closely tied to HR processes, employee onboarding/off-boarding, and access management.  To SOC or Not To SOC - That is the Question Deciding whether to establish a Security Operations Center (SOC) is a significant decision that can shape your organization\u0026rsquo;s entire approach to cybersecurity. While the idea of having a dedicated team working around the clock to monitor and respond to threats is appealing, the decision should be based on a nuanced understanding of your organization’s unique characteristics and needs. Here\u0026rsquo;s what I\u0026rsquo;d recommend considering as you ponder your organizational structure:\nHistorical Team Capabilities Begin by assessing your current team’s capabilities. Have they historically been able to handle security incidents efficiently? Do they possess the technical skills required to monitor, detect, and respond to threats in real-time? Do you already have a SOC and are deciding to reorg? Are you a new company building your first \u0026ldquo;Blue Team\u0026rdquo;? An organization with a mature and capable security team might find that building out a SOC could amplify their strengths, enabling more robust and effective threat response. Meanwhile, a company with a greenfield security structure may decide to start SOC-less and build an organization around that model.\nOrganizational Dynamics Consider how a SOC fits into your organizational structure. Do you have the support from executive leadership and the necessary budget to implement and sustain a SOC? It’s also important to evaluate how a SOC would integrate with existing teams. Will it create silos, or can it facilitate better communication and coordination across departments? Do other teams want to take on the work, and do they have the necessary funding?\nAutomation Sophistication Evaluate the level of automation in your current security processes. A highly automated environment can reduce the need for a large, traditional SOC by enabling more efficient threat detection and response. Automation tools can handle routine monitoring and alerting, allowing your security team to focus on more complex tasks.\nCulture of Ownership Your organization\u0026rsquo;s culture plays a crucial role in deciding whether a SOC is the right choice. A strong culture of ownership, where teams are empowered and responsible for their security posture, can reduce reliance on a centralized SOC. In such environments, a distributed security model with embedded security champions might be more effective.\nThe Decision Ultimately, the decision to establish a SOC should be guided by a strategic evaluation of these factors. For some organizations, a SOC provides the necessary focus and resources to stay ahead of threats. For others, leveraging automation and fostering a culture of security ownership might be a better path. The key is to align your security operations with your organization\u0026rsquo;s strengths and needs.\nConclusion By recognizing these distinct yet interconnected pillars and their unique characteristics, we can develop more targeted and effective security strategies. Each pillar requires its own set of tools, expertise, and response protocols. Whether you choose to implement a 24/7 SOC team for threat detection or distribute responsibilities across teams, understanding your organizational dynamics and capabilities is key.\nIn practice, these pillars work together synergistically. A threat detection alert might trigger a broader vulnerability scan, while a compliance violation could lead to updates in security posture management. By understanding and implementing all pillars of security monitoring, and carefully considering your approach to SOC, we can build a more resilient and adaptive security posture.\nAs cyber threats continue to evolve in complexity and scale, maintaining a clear understanding of these distinct yet interconnected monitoring types becomes increasingly crucial. It\u0026rsquo;s not just about having multiple layers of security – it\u0026rsquo;s about having the right layers, working in harmony to provide comprehensive protection.\nIn the end, while all security monitoring aims to prevent unauthorized access, the path to achieving this goal is multifaceted. By recognizing and leveraging the unique strengths and response patterns of each pillar, and making informed decisions about your SOC strategy, we can build a more robust, efficient, and effective security strategy. In the ever-changing world of cybersecurity, this nuanced approach isn\u0026rsquo;t just beneficial – it\u0026rsquo;s essential for staying ahead of evolving threats and protecting our critical assets.\n",
    "ref": "/blog/typesofsecuritymonitoring/"
  },{
    "title": "Detection as Code: A Maturity Framework",
    "date": "",
    "description": "Detection as Code (DaC) is transforming how we handle threat detection by merging software engineering practices with security operations. Breaking down DaC into functional areas can enhance your organization's detection capabilities and maturity.",
    "body": "Detection as Code: A Maturity Framework Introduction In the ever-evolving landscape of cybersecurity, the methodologies and tools we rely on are continually advancing. One of the latest trends is Detection as Code (DaC), which promises to revolutionize threat detection by integrating software engineering practices into security operations. Having worked in an organization that practices DaC for several years, I\u0026rsquo;ve come to realize that it\u0026rsquo;s not simply a matter of whether or not to adopt DaC.\nIf we want to adopt DaC, where do we start? And how do we know if we\u0026rsquo;re doing a good job at it? How do I convince leadership that it\u0026rsquo;s an area we should invest in? At Snowflake, we\u0026rsquo;ve been executing on aspects of DaC long before I arrived. However, we quickly learned that writing our detections in an IDE wasn\u0026rsquo;t sufficient to achieve the operational results we wanted. Like most principles in cybersecurity, DaC can be broken down into multiple functional areas, each with its own levels of maturity. I\u0026rsquo;ll share my insights into these functional areas, the various maturity levels within them, and how to improve your competency in each area.\nUnderstanding Detection as Code Detection as Code (DaC) leverages automation and version control to manage, deploy, and maintain detection rules, policies, and configurations. By treating detection rules as code, DaC enables rigorous testing, continuous integration/continuous deployment (CI/CD), and monitoring practices to enhance the reliability and efficiency of threat detection. A key aspect of DaC isn\u0026rsquo;t just writing detections in YAML or Python, but also applying the same development and deployment rigor as software engineers.\nFunctional Areas and Maturity Levels DaC can be divided into four functional areas: programming language, testing, integration \u0026amp; automation, and monitoring. Each area has four maturity levels: Ad hoc, structured, automated, and optimized. Let’s explore each area in detail.\nProgramming Language The choice of language for defining detections significantly impacts capabilities. Languages vary in flexibility and tools for translating ideas into code. Many teams start with markup languages, such as YAML. YAML has almost no barrier to entry and is extremely human readable.\nImage Source: https://github.com/SigmaHQ/sigma/blob/master/rules/cloud/aws/cloudtrail/aws_cloudtrail_disable_logging.yml\nThe Sigma Project is a well-known DaC project focused on YAML. However, expressing complex logical thoughts with YAML can be challenging.\nA step up from YAML are query languages such as SQL or SPL. Query languages are specifically designed for analyzing large amounts of systems from databases or other data stores (such as one where you may keep your logs). The SIEM I\u0026rsquo;m most familiar with that is purely based on using SQL is SnowAlert (a now mostly deprecated project).\nGeneral-purpose languages like Python offer the most flexibility in translating detection logic to code. Many modern detection engineering teams are moving towards Python or Go for interfacing with their data due to their flexible structures and rich libraries.\nAn optimized team uses the best tool for the job, seamlessly integrating markup, query, and functional languages.\nMaturity Levels  Ad hoc: Markup language Structured: Query language Automated: General-purpose language Optimized: Any combination of the above/best tool for the job  Testing Testing your detections is a critical and sadly often neglected area. I\u0026rsquo;ve broken down testing into five different areas (unit testing, linting, performance testing, integration testing, and end-to-end), which can seem even more daunting. You may say to yourself \u0026ldquo;that\u0026rsquo;s such a huge task, why even bother?\u0026rdquo; It\u0026rsquo;s important to realize that significant improvements can be realized with just a modicum of testing.\nUnit Testing Unit testing is focused on ensuring your detection logic is correct that works by passing input to your detection logic and ensuring the output matches as expected.\nIn the above example based on Panther, we pass an example input into our rule along with an expected output.\nLinting Linting is basic static analysis for code quality. Linting can speed up code reviews and ensure highly standardized code even with multiple engineers contributing.\nImages source: https://plainenglish.io/blog/how-to-lint-properly-your-sql-code-5ae21e6f9f5c\nPerformance Testing Performance testing will run your queries or detections and ensure they run in a timely and cost efficient way.\nIntegration Testing Integration testing ensures that disparate systems which interface together properly interface as expected. This is most common when you have other parts of your detection platform such as SOAR.\nEnd-to-End Testing End-to-end testing occurs when you full test your system from data generation all the way to case management and potentially closure.\nImage Source: https://github.com/WithSecureLabs/leonidas\nTesting Priority and Difficulty Not all testing methodologies are created equal. Some tests are hard to get setup or maybe aren\u0026rsquo;t a super high priority. For your organization, you should decide on the areas you\u0026rsquo;re seeing the most operational failures and work to setup testing for those areas. Note that the below difficulty may vary depending on your tech stack and organizational structure.\n   Test Type Priority Difficulty     Unit Testing High Low   Linting Low Low   Performance Medium Medium   Integration Medium Medium   End-To-End Low High    Maturity Levels  Ad hoc: No or minimal testing Structured: Some testing in all areas or strong testing in some areas but missing in others Automated: Some testing in all areas with strong capabilities in a few categories. A sandbox/dev environment exists and is used Optimized: Appropriate testing for all components with code coverage measurements. Tests required for new detections, with appropriate mechanisms in place (ie. robust criteria and automated expiration) for break-glass scenarios  Automation and Integration Developing appropriate automation and integrations is critical to improving analyst response time and decrease error likelihood. Anytime your SOC analyst has to context switch, go into another system for information, or look something up, it increases time to respond and adds unnecessary friction. I firmly believe that as detection engineers, our job is to give as much information to the SOC so they can do what they do best: analysis and response. Integrations empower your analyst and therefore should be a top priority for any robust detection team.\nSuppression One of the most common detections is \u0026ldquo;impossible travel\u0026rdquo;. When an analyst gets this alert, the first step in their playbook is usually to reach out to the user to confirm that it was in fact them logging into a system. We can use automation to suppress this alert, though.\nImage Source: https://www.tines.com/blog/chatbots-for-security-and-it-teams-part-3-creating-a-slack-chatbot/\nIf the user clicks \u0026ldquo;I recognize this activity\u0026rdquo; then the alert is closed without further (or any) action needed from the analyst.\nEnrichment Sometimes the data you need doesn\u0026rsquo;t exist in your logs because maybe it\u0026rsquo;s too expensive or too infeasible to ingest for whatever reason. In this case, you\u0026rsquo;ll want integrations that allow you to query real-time data in order to enrich an alert. Looking at the example where one can escalate privileges through EC2 user data, we can see that we can detect this activity through watching CloudTrail for the \u0026ldquo;ModifyInstanceAttribute\u0026rdquo; event. However, the actual user data is removed because its very common for secrets to be in this data.\nSo an analyst\u0026rsquo;s first step would be to run a command to pull this data.\nHowever, ran than rely on an analyst to have all the necessary permissions and access, the better solution is to fold these capabilities into your detection platform and automatically run this command when the alert fires and appropriately enrich your alert. Thus, you have a more secure and faster system that allows your responders to do their analysis in a timely and efficient way.\nGeneration This may be the most common integration that detection engineers will support in their environment. Let\u0026rsquo;s say you use a Cloud Detection and Response tool. While a basic implementation may involve directly sending alerts to your case management system, a mature DaC team will ingest those alerts into their SIEM just like any other data. From there, alerts can be normalized, suppressed, and enriched as appropriate.\nMaturity Levels  Ad hoc: No or minimal integrations Structured: Some integrations setup, but setting up new integrations is high effort. Missing tooling and standards Automated: Process and tooling exists for developing new integrations, can automation suppression or enrichment for key detections Optimized: Integrations for suppression and enrichment. Can easily incorporate 3rd party alerts. Strong standards and tooling for deployment of new integrations  Monitoring As our systems grow in complexity, it becomes increasingly important to monitor their inputs, activity, and outputs. Identifying and resolving failures or errors in your system as quickly as possible is the goal. I generally break down monitoring into 3 parts: inputs, executions, and outputs. Are you getting good data? Are you monitoring the performance or run time failures in your system? Are you generating useful alerts?\nInputs Data quality can be (and has been many times from other writers) its own blog post. For the purpose of detection engineering, I focus on five areas of data quality:\n Format or ensuring your data is well structured and parsable (please stop using syslog) Volume or monitoring for spikes or drops in data volume Completeness or ensuring you\u0026rsquo;re collecting data from all systems or accounts Timing or ensuring that data is reaching your detection engine soon after the event occurs Data contracts or your ability to work with system owners to ensure they improve their system in a timely manner if any area of data quality drops  Execution Monitoring the runtime of your system can further be broken down into two main categories.\nPerformance/Cost Are you watching for spikes in cost or drops in performance? Do you find out about an issue when you get the bill or have you setup monitoring to rapidly alert to cost spikes?\nFailure Handling This has large overlap with business continuity and disaster recovery and will make your compliance team happy. When a rule fails, does your system identify this and re-run it automatically? Do you get alerted when it repeatedly happens so you can take action? Or do things silently fail? If you wrote all these great detections but something is silently failing, then you\u0026rsquo;re not executing an effective detection engineering system.\nOutputs This is the area of monitoring that most security operations are familiar with and start off with: labeling your alerts as true or false positive and working to improve the false positives. As you mature, you can grow in this area with improved labeling or proactively suppress alerts that start spiking and overwhelming your triage team. As an example, at Snowflake, we\u0026rsquo;ll quarantine a particular detector and create a ticket for our detection engineers if it has too many false positive closure statuses within a particular time window.\nMaturity Levels  Ad hoc: Find out about missing data during forensics, get told about detection issues from the SOC Structured: Collecting output metrics, basic dashboards and visualizations Automated: Notified about failures, established SLAs and SLOs Optimized: Automated suppressions, data/performance contracts, automatic recovery capabilities  Maturity Matrix     Language Testing Integration Monitoring     Ad Hoc Markup language No or minimal testing No or minimal integrations Find out about missing data during forensics, get told about detection issues from the SOC   Structured Query language Some testing in all areas or strong testing in some areas but missing in others Some integrations setup, but setting up new integrations is high effort. Missing tooling and standards Collecting output metrics, basic dashboards and visualizations   Automated General-purpose language Some testing in all areas with strong capabilities in a few categories. A sandbox/dev environment exists and is used Process and tooling exists for developing new integrations, can automation suppression or enrichment for key detections Notified about failures, established SLAs and SLOs   Optimized Any combination of the above/best tool for the job Appropriate testing for all components with code coverage measurements. Tests required for new detections, with appropriate mechanisms in place (ie. robust criteria and automated expiration) for break-glass scenarios Integrations for suppression and enrichment. Can easily incorporate 3rd party alerts. Strong standards and tooling for deployment of new integrations Automated suppressions, data/performance contracts, automatic recovery capabilities    Setting Goals You may read through this blog post and wonder how you\u0026rsquo;ll ever reach optimized in all categories. It\u0026rsquo;s my belief that this shouldn\u0026rsquo;t be the goal for all teams. Security is ultimately a function of risk management, and you shouldn\u0026rsquo;t overinvest for how much risk you mitigate. Similar, with adopting devops principles your detections, you may end up over-engineering when resources could have been better invested in other areas. As an example, if you\u0026rsquo;re a smaller shop where your only detection engineer also runs vulnerability management and incident response, then shooting for optimized across the board may not be realistic. In that case, it may be more important that you have strong integrations and automation to scale your engineer. Meanwhile, a shop that has a ton of legacy components may want to invest in monitoring to ensure all areas are working as expected. Reason through your particular situation and where your pain points to decide where on the matrix you want to end up before setting unreachable goals.\nConclusion Detection as Code (DaC) represents a significant advancement in cybersecurity, bringing software engineering principles into threat detection processes. By breaking DaC down into functional areas—programming language, testing, integration \u0026amp; automation, and monitoring, we can assess our current maturity and pinpoint areas for improvement.\nEach functional area offers a pathway from basic, ad hoc practices to optimized, comprehensive systems that enhance efficiency, reliability, and response times. Implementing DaC requires a commitment to continuous improvement, leveraging the best tools and practices available.\nAs cybersecurity threats continue to evolve, adopting DaC can provide a robust framework for staying ahead. By treating detection rules as code, integrating rigorous testing, and ensuring seamless automation and monitoring, we can significantly enhance our security posture.\nWhether your team is just starting out with DaC or looking to optimize an existing system, focusing on these key areas will help you build a more resilient and responsive security operation. The journey to maturity in DaC is incremental, but the benefits—reduced response times, fewer errors, and more effective threat detection—are well worth the investment.\n(Note, this is a blog post version of a talk I gave a while ago with some additional updates. You can find the original slides here.)\n",
    "ref": "/blog/detectionascodematurity/"
  },{
    "title": "There are Only Two Rules",
    "date": "",
    "description": "One mental model I've maintained for a long time is that there are only two types of detection rules. We'll cover what those rules are and why this distinction matters.",
    "body": "Intro In the field of detection engineering, understanding and applying the right frameworks is crucial for effectively identifying and responding to threats. Mental models provide a structured way of categorizing and approaching the myriad of security alerts and anomalies we face daily. These models help us dissect and understand the behavior of potential threats, allowing for a more targeted and effective defense strategy. In this blog post, I\u0026rsquo;ll explain how I use one mental model to clearly classify every detection rule into one of two categories and how that helps shape the decision of what kind of detections I build.\nThe Two Rules Essentially, every detection can be categorized as an \u0026ldquo;allowlist\u0026rdquo; detection or a \u0026ldquo;denylist\u0026rdquo; detection. An allowlist detection is written to state that only a certain set of actions are allowed and to alert on anything that isn\u0026rsquo;t in that list of allowed actions. Conversely, a denylist detection looks for specific actions and alerts if they occur. Said another way, an allowlist detection looks for anything except a specific set of actions occurring and alerting if something not in that set happens while a denylist detections alerts if a specific stated behavior occurs. And all threat detection rules can be cleanly categorized as one or the other.\nDefending the Thesis Let\u0026rsquo;s take a look at some example rules and establish how (in my experience) every rule can be classified into one of these categories.\nPort Scanning Port scanning is an obvious candidate for denylist categorization. Most port scanning alerts look for a specific number of connections matching a specific pattern of behavior with regards to ports in some way (usually number of ports, diversity of ports, a time window, TCP flags, etc are involved in the pattern). One can see that since we\u0026rsquo;re looking for a set of actions that matches a pre-defined list, that meets our definition of a denylist detection.\nIAM Role Abuse Let\u0026rsquo;s say we have a detection that monitors a specific AWS IAM role. It\u0026rsquo;s a critical role with high permissions for a service. So we write a rule that says the role policy must match a predefined set of permissions, the role can only be assumed from a certain IP address, the user agent must match a predefined string, etc. In this case we\u0026rsquo;ve built a list of conditions that are allowed and if any condition does not match, we will alert. This matches our definition of an \u0026ldquo;allowlist\u0026rdquo; detection.\nMalware Detections for malware clearly fall in the denylist category. Whether behavior based, signature based, or other - they look for a specific signal and alert if it occurs.\nML Models Even ML models can generally be categorized as allowlist or denylist. EDRs such as CrowdStrike rely on learned behavior of what bad looks like (denylist) while other tools that claim to \u0026ldquo;baseline\u0026rdquo; your environment rely on definitions of good or \u0026ldquo;allowlists\u0026rdquo;. While the sources that feed into what\u0026rsquo;s good and bad are a bit more dynamic, the general categorization still applies.\nOther Let\u0026rsquo;s take a quick sampling of Elastic\u0026rsquo;s prebuilt rules and see if we can categorize them.\nAWS CloudTrail Log Deleted - Denylist\nAWS EC2 Snapshot Activity - Denylist\nLSASS Process Access via Windows API - Denylist\nOnce you understand the distinction, it\u0026rsquo;s pretty easy to categorize a rule.\nATT\u0026amp;CK vs D3FEND One thing I\u0026rsquo;ve noticed as I\u0026rsquo;ve categorized our own internal rule set is that Denylist rules generally match up better with ATT\u0026amp;CK whereas Allowlist rules are generally categorized by D3FEND. Let\u0026rsquo;s go over our above examples again.\nPort Scanning This lines up fairly cleanly with Active Scanning: Scanning IP Blocks - T1595.001 and Network Service Discovery - T1046.\nIAM Role Abuse Let\u0026rsquo;s say we\u0026rsquo;ve build a detection that does the following:\n Monitors the IPs where the role is used Monitors user agent string Monitors what actions the role performs.  The first lines up with User Geolocation Logon Pattern Analysis.\nThe second, it can be argued, lines up with Protocol Metadata Anomaly Detection as we\u0026rsquo;re monitoring metadata about the network traffic and looking for deviations.\nAnd the last matches up with Resource Access Pattern Analysis as we\u0026rsquo;re understanding what resources (or actions) the role is performing.\nIf you were to read a pentest report where a red teamer gained access to a role and abused it, they may categorize that action under T1552.005 - Unsecured Credentials: Cloud Instance Metadata API or Valid Accounts: Cloud Accounts - T1078.004. But that\u0026rsquo;s not really what our detector is looking for, is it? That\u0026rsquo;s one of the reasons I firmly believe over-categorizing detections with ATT\u0026amp;CK is a fallacy, but that\u0026rsquo;s a monologue for another blog.\nWhile D3FEND doesn\u0026rsquo;t have quite the popularity of ATT\u0026amp;CK, I still find it especially useful for detection engineers to categorize their rules, and we\u0026rsquo;ve even created our own Snowflake D3FEND techniques internally for further categorization and sub-categorization.\nTrends One trend you may notice as you go through your own internal rule corpus and begin to categorize their actions is that the vast majority of your rules are denylist based or ATT\u0026amp;CK lined. Why do you think that is? I have a few theories. First, if you buy any tool or go to any source of free rules, they\u0026rsquo;re almost all going to be denylist based. So right away, any company is going to have a decent amount of denylist detections by default. As junior engineers come up, they\u0026rsquo;re going to see these types of rules and emulate them in their own authorship. But that doesn\u0026rsquo;t really answer the question of why default to denylist?\nEasier to Write A denylist detection is inherently going to be easier to write. Denylist detections are based on adversary knowledge as opposed to knowledge about an internal environment. As a detection engineer, there\u0026rsquo;s a lower barrier to entry. You don\u0026rsquo;t have to go talk to other teams. You study the adversary, understand the behavior, and write the rule. You may have to suppress some false positives specific to your environment, but you can generally just author and go.\nMore General The above point leads to these rules being more general and easier to share (one of the reasons that almost every public rule is denylist based). Let\u0026rsquo;s take a look at one of the example Elastic rules:\nfile where host.os.type == \u0026quot;windows\u0026quot; and event.action != \u0026quot;deletion\u0026quot; and\r/* MZ header or its common base64 equivalent TVqQ */\rfile.Ext.header_bytes : (\u0026quot;4d5a*\u0026quot;, \u0026quot;54567151*\u0026quot;) and\r(\r/* common image file extensions */\rfile.extension : (\u0026quot;jpg\u0026quot;, \u0026quot;jpeg\u0026quot;, \u0026quot;emf\u0026quot;, \u0026quot;tiff\u0026quot;, \u0026quot;gif\u0026quot;, \u0026quot;png\u0026quot;, \u0026quot;bmp\u0026quot;, \u0026quot;fpx\u0026quot;, \u0026quot;eps\u0026quot;, \u0026quot;svg\u0026quot;, \u0026quot;inf\u0026quot;) or\r/* common audio and video file extensions */\rfile.extension : (\u0026quot;mp3\u0026quot;, \u0026quot;wav\u0026quot;, \u0026quot;avi\u0026quot;, \u0026quot;mpeg\u0026quot;, \u0026quot;flv\u0026quot;, \u0026quot;wma\u0026quot;, \u0026quot;wmv\u0026quot;, \u0026quot;mov\u0026quot;, \u0026quot;mp4\u0026quot;, \u0026quot;3gp\u0026quot;) or\r/* common document file extensions */\rfile.extension : (\u0026quot;txt\u0026quot;, \u0026quot;pdf\u0026quot;, \u0026quot;doc\u0026quot;, \u0026quot;docx\u0026quot;, \u0026quot;rtf\u0026quot;, \u0026quot;ppt\u0026quot;, \u0026quot;pptx\u0026quot;, \u0026quot;xls\u0026quot;, \u0026quot;xlsx\u0026quot;, \u0026quot;hwp\u0026quot;, \u0026quot;html\u0026quot;)\r) and\rnot process.pid == 4 and\rnot process.executable : \u0026quot;?:\\\\Program Files (x86)\\\\Trend Micro\\\\Client Server Security Agent\\\\Ntrtscan.exe\u0026quot;\rWe can clearly see it\u0026rsquo;s looking for some header bytes with specific file extensions. We also see some exclusions at the end. It\u0026rsquo;s incredibly easy for a detection engineer to run this in their environment, observe the false positives, and extend the excluded processes at the end.\nStrengths and Weaknesses Now that we\u0026rsquo;ve established these broad categorizations, let\u0026rsquo;s take a look at the strengths and weaknesses of allowlist and denylist detections.\nDenylist As we\u0026rsquo;ve established earlier, denylist detections are easier to write. The reason is that it requires knowledge of the adversary and their behavior with minimal specific knowledge of your own environment besides what is necessary to tune false positives. As a denylist rule looks for a specific pattern of behavior, it tends to have a much lower ratio of false positives. The corollary to this, however, is that denylist rules also will have a broader range of false negatives. Let\u0026rsquo;s go back to our port scanning detection as an example.\nSELECT source_ip,\rCOUNT(DISTINCT destination_port) AS distinct_ports_accessed,\rMIN(timestamp) AS scan_start,\rMAX(timestamp) AS scan_end\rFROM firewall_logs\rWHERE timestamp \u0026gt;= '2023-01-01' -- adjust the date range as needed\rAND timestamp \u0026lt; '2023-02-01'\rGROUP BY source_ip\rHAVING COUNT(DISTINCT destination_port) \u0026gt; 100 -- threshold for number of distinct ports\rAND MAX(timestamp) - MIN(timestamp) \u0026lt; INTERVAL '1 HOUR' -- within a short time frame, adjust as needed\rORDER BY distinct_ports_accessed DESC;\rHere we have a basic detection (written by ChatGPT) that looks in our firewall logs for more than 100 distinct ports accessed by a source IP address in a 1 hour time frame. If this rule fires, while it may be a benign true positive, it will have alerted to the behavior we wanted it to. Now, one can quickly look at the code and see that there\u0026rsquo;s numerous instances a false negative can occur:\n If the adversary rotates their source IP address If the adversary only scans 99 ports If the adversary scans slowly so no more than 100 distinct ports are hit within a 1 hour time frame  Finally, denylist rules are fairly easy to test. There\u0026rsquo;s numerous tools to help you simulate malicious activity such as atomic red team.\nAllowlist The strengths and weaknesses of allowlist and denylist rules essentially flipped. First of all, writing a good allowlist rule requires specialized knowledge of an environment: what actions does this service account do, what machines does this server normally connect to, who are our admins and what\u0026rsquo;s the process for adding them, etc. It requires threat detection engineers to work with system owners to understand the risks to the systems and how they generally behave. Prioritization also becomes more difficult as it requires a holistic understanding of your risks and how effectively you can mitigate them with detections. Additionally, false positives increase while false negatives decrease. Let\u0026rsquo;s work through an example again.\nSELECT eventtime,\reventname,\rsourceipaddress,\ruseridentity,\rrequestparameters\rFROM cloudtrail_logs\rWHERE useridentity.role = 'billing_service_role' -- role involved in the access\rAND eventtime \u0026gt;= '2023-01-01' -- specific time range of interest\rAND eventtime \u0026lt; '2023-02-01'\rAND (\rNOT sourceipaddress IN ('192.168.1.1', '192.168.1.2') -- allowed EC2 IP addresses\rOR eventsource != 'rds.amazonaws.com'\r)\rORDER BY eventtime DESC;\rIn the above query, we\u0026rsquo;re looking at fake role called billing_service_role and alerting if it\u0026rsquo;s not coming from a source IP address associated with our internal IP addresses or if it\u0026rsquo;s not performing an action associated with the AWS RDS service. In this theoretical example, we\u0026rsquo;ve based this off of analysis and discussions with the engineering team that manages this role. In this case, we expect very few false negatives. If this role is compromised and used from another IP or if it\u0026rsquo;s compromised on the instance and any kind of enumeration or other activity is performed, we\u0026rsquo;d expect to get an alert. However, if the engineering team scales up to 3 instances without telling us or if they expand the capabilities of this service account, we can expect to quickly be overwhelmed with false positives.\nTable Summary     True Positives False Positives Requires Internal Knowledge? Easy to Test?     Allowlist High High Yes No   Denylist Low Low No Yes    When to Use Which In order to understand which style of rule to write, I generally use a few guiding questions:\nIs this an internally defined risk or externally defined?\n Externally defined threats often lend themselves to denylist style rules  How much do we understand the system at risk?\n Allowlist detections require an understanding of the internal system  How often do we expect the system or threat to change?\n Highly variable systems are not generally good for allowlist rules unless you can dynamically update your allowlist  Is the detection looking to defend something early or late in the attacker lifecycle? (i.e., how close to the \u0026ldquo;crown jewels\u0026rdquo; is the system we\u0026rsquo;re defending with this detection?)\n Being close to crown jewels means you may want to strive for a rule with very low false negatives i.e. an allowlist rule  If a false positive occurs, would we be able to easily deduplicate it to avoid flooding our SOC?\n If something is hard to deduplicate, you may want to avoid allowlist which can generate high false positives  How \u0026ldquo;expensive\u0026rdquo; to our SOC is a false positive? (i.e. does it take a lot of time and resources to investigate false positives?)\n Expensive triage means you probably want to avoid high false positives  These are just guiding questions and you may find that the answers don\u0026rsquo;t lend to a clean decision. In some cases, we\u0026rsquo;ve built both allowlist and denylist rules for a specific risk.\nBuild vs Buy I\u0026rsquo;d like to touch on a concept that\u0026rsquo;s dear to my heart and related to this conversation: the decision to build or buy something. As a detection engineer, when we think about building or buying tools, we\u0026rsquo;re really building or buying detections. Let\u0026rsquo;s take Gem Security. In addition to other capabilities, they provide a few hundred out of the box detections for cloud providers as well as other solutions. Like most vendors, they\u0026rsquo;re rules are primarily denylist focused. When choosing to go with a vendor like this, what you\u0026rsquo;re really deciding is if it\u0026rsquo;s worth it for your organization to buy these detections or build them yourself. Fundamentally, when you choose to purchase or not purchase solutions like these, you\u0026rsquo;re making a decision on what is the most important thing to invest in. If you have a large corpus of internal risks that need mitigating via detections, it may behoove you to buy detections like these. In general, I\u0026rsquo;ve noticed that as organizations mature, they actually buy detections more than build. The reason for that is that their internal risk identification apparatus has identified numerous areas that need detections. If you\u0026rsquo;re going to hire smart detection engineers, in my opinion, it makes more sense for them to build things unique to your organization as opposed to reinventing the wheel.\nConclusion In this piece, I\u0026rsquo;ve stated my definitions for the critical frameworks of allowlist and denylist detections, each with its distinct characteristics and applications. These are not mere theoretical constructs; they are practical tools that help us navigate the complex landscape of cybersecurity threats and defenses. They force us to ask the right questions about our systems, understand our adversaries, and articulate our defensive strategies with clarity and precision.\nThe choice between allowlist and denylist is a strategic one, reflecting our understanding of the threat landscape and our systems\u0026rsquo; architecture. It\u0026rsquo;s about making informed choices, leveraging the strengths of both methodologies to build robust and resilient defenses. As you continue on your journey as a detection engineer, I hope you use this framework to make informed and justifiable decisions as you work towards building more secure systems.\n",
    "ref": "/blog/thereareonlytworules/"
  },{
    "title": "Building Resilient Detection Suppressions",
    "date": "",
    "description": "When a false positive occurs, how do you approach tuning your detector? Do you allowlist that one event? That can lead to another false positive tomorrow. This article will provide an example of how we build resilient suppressions.",
    "body": "Intro Raise your hand if you\u0026rsquo;ve worked on suppressing a false positive for a detector and then the very next day, it\u0026rsquo;s back in your \u0026ldquo;fix\u0026rdquo; queue with another false positive. I\u0026rsquo;d expect almost everyone who\u0026rsquo;s worked in detection engineering for any period of time has their hand raised or is nodding along. Why does this happen? Well, sometimes the detector itself isn\u0026rsquo;t very good and it\u0026rsquo;s alerting on a lot of benign behavior. But sometimes, the answer is that the suppression that was implemented was too specific and wasn\u0026rsquo;t properly generalized to capture the benign behavior.\nAt Snowflake, when we write our suppressions, we strive to understand the root cause that drove the false positive so that our detections are resilient and don\u0026rsquo;t immediately pop back up in the queue for another tweak. In this article, I\u0026rsquo;ll walk through my thought process on a recent suppression request that came into our queue.\nThe Detection Like most cloud based companies, we have numerous rules that monitor IAM activity in our cloud environments. One particular rule is focused on monitoring the behavior of some very important roles that orchestrate a lot of backend services. During development of this system, we worked with our product security and cloud engineering to orchestrate detections around how these roles are used. We built a static list of roles and the actions they performed and created an \u0026ldquo;allowlist\u0026rdquo; style detection. Essentially, we said that we expect this set of roles to only perform these actions. Anything that deviates from this behavior should generate an alert. We understood that these are more fragile, but considering the importance of the service, we decided that the balance played in favor of security.\nThe False Positive When the benign activity occurred, someone had modified the backend application to use the role to perform a new action. We were able to verify with the engineers and the pull request that the behavior we were seeing in the logs was, in fact, benign. We also verified by examining the permissions granted to the role that it was authorized in the IAM policy.\nThe Suppression Now it came time for us to decide how to alter the detector so that we wouldn\u0026rsquo;t see this type of false positive again.\nApproach 1 The most obvious approach is to extend our list of allowed methods to include the new method. However, we quickly realized we\u0026rsquo;d run into more benign alerts if someone wrote another module with new capabilities, which certainly happens.\nApproach 2 The next idea was to see if we could dynamically build an allowlist of methods based on the permissions granted to the role. We took a look at the grants to the role and realized that wouldn\u0026rsquo;t be feasible. Considering the role was used to orchestrate the environment, it had quite a range of permissions granted to it.\nApproach 3 We decided to pivot and say: let\u0026rsquo;s assume the role can do anything. What if we focus our efforts on making sure the role is being used from the correct compute nodes? (We have separate monitoring on validating the security of those nodes.) Then our assumptions can be:\n These roles are orchestrated by a service running in these compute nodes This service is controlled by our CI/CD systems with code living in GitHub The compute nodes are secure via monitoring and various other controls Our repo is secure via codeowners, protected branches, etc  Thus, we can state that so long as the role is being used from the instance, it\u0026rsquo;s being used correctly. To accomplish this, we can look at our asset inventory and build a list of internal IPs associated with the cluster running this service at detection run time. Furthermore, we realized that we should update our list of static roles to be dynamic. Again, we used our asset inventory to pull a list of roles rather than our static list. To ensure our list of roles was accurate, we based our filtering on tags applied to the roles.\nAfter implementing these changes, we updated our detector, tested it, and pushed it back to production.\nConclusion When building dynamic suppressions, it\u0026rsquo;s important to consider several factors, with the key focus being on ensuring that all elements that can be made dynamic are indeed implemented as such. In our example, we realized that our list of IPs and roles should be more dynamic to yield a more resilient detector. In addition, by understanding our threat model and other compensating controls, we were able to remove a condition that was prone to inducing benign false positive behavior. Hopefully this short article will help you the next time you come across a false positive.\n",
    "ref": "/blog/buildingresilientsuppressions/"
  },{
    "title": "How to Write an Actionable Alert",
    "date": "",
    "description": "Writing a powerful detection is great, but if your SOC/IR team can't act upon it, how useful is it really? This article will serve as a guide on how to write an actionable alert.",
    "body": "Intro \u0026ldquo;If a tree falls in a forest and no one is around to hear it, does it make a sound?\u0026rdquo;\nThis mantra about possibly non-existent trees very much applies to detection engineers. If you write a great rule to detect malicious behavior, but no one (or nothing) acts upon the alert, did you really catch the threat actor? Or worse, an analyst acts on the alert but responds incorrectly. In this article, I will cover the importance of not only writing solid detections, but also solid alerts. The target audience of this blog post is security professionals who write custom detections in their SIEM or other similar system.\nDefinitions Before we get much further, I\u0026rsquo;d like to clarify how I define a rule as opposed to an alert. In my lexicon, a rule (or query) is the construct by which one searches for a behavior (often malicious) within a data set. This rule can be streaming or scheduled. The alert, on the other hand, is the output of that rule. A rule and data will enter a SIEM and the output is an alert generating a ticket or case in a downstream system.\nWriting a Good Rule There\u0026rsquo;s been a number of great blog posts and articles that talk about how to write a good rule. I\u0026rsquo;ll try to briefly summarize a few of the pieces that inspired me. (If you\u0026rsquo;re generally unfamiliar with detection engineering, I highly suggest you read these articles as it will help your understanding with the rest of this article.)\nPyramid of Pain I think any article on this topic would be remiss if it didn\u0026rsquo;t start by referencing the Pyramid of Pain. This piece by David Bianco has really pushed detection engineering forward to where detection engineers now strive to move beyond the most basic indicators and towards the top of the pyramid - TTPs.\nFunnel of Fidelity Next up is one of my favorite pieces on the Funnel of Fidelity. This series of pieces covers the straining of data to find the pieces that accurately represent the behavior you\u0026rsquo;re looking for. In general, I recommend all of SpecterOps and Jared Atkinson\u0026rsquo;s pieces. They\u0026rsquo;re well worth a read.\nDetection Development Lifecycle It can never hurt to sprinkle in some personal bias. I think the Detection Lifecycle that we utilize at Snowflake is a pretty good place to start when it comes to learning about building solid detections.\nWhat Makes a Good Alert So now that we know what makes a good rule, let\u0026rsquo;s dive into what makes a good alert. In my experience, there are a few qualities that make an alert good.\n  Immediately human actionable. If you as a detection engineer are going to present an alert, the analyst who\u0026rsquo;s responding to it should have the tools and information they need to immediately make a decision and take an action.\n  Automatically enriched. If the first step an analyst needs to take when triaging an alert is to collect additional data or information, I consider this a failure.\n  Well prioritized. When presented with a slew of alerts to work, the analyst should be able to clearly identify which is the most important to work first and why.\n  Grouped and correlated. If multiple different alerts trigger for a system or user, they should be grouped and correlated so the analyst(s) working the case doesn\u0026rsquo;t miss that multiple related alerts exist.\n  Let\u0026rsquo;s dive into how to achieve these properties.\nHuman Actionable In my opinion, one of the most important properties a good alert should have is that it needs to be \u0026ldquo;human\u0026rdquo; actionable. For this to be true, a human needs to be able to review the content of the alert, know what their next steps should be, and have the tools necessary to take those next steps. A key qualifier in this section is \u0026ldquo;human\u0026rdquo; actionable. A good alert should require a human to make a decision as the next step in the response process. This differs from \u0026ldquo;machine\u0026rdquo; actionable where the next steps could be automated or executed by code. Let\u0026rsquo;s take a look at a bad alert that does not demonstrate this property.\nTitle: Login from New Device User: jane.doe@snowflake.com Details: Identified user logged in from a new device. IPs were 123.34.12.11. Timestamps: 14:33 UTC Playbook: Reach out to user and confirm if the login was from them. If not, proceed to reset their password. Let\u0026rsquo;s assume that the rule itself is accurate and the login was from a new device. What\u0026rsquo;s wrong with this alert? Well the first step is to reach out to the user. This step is automatable and does not require a human to take the next step. When you\u0026rsquo;re designing your alert, ask yourself, \u0026ldquo;Do we require the intelligence and ingenuity of our human analysts to make a decision or act on this information?\u0026rdquo; If the answer is no, then more should be done to improve the alert.\nOn the subject of automation, I personally believe that any automation that is designed to improve the fidelity or the quality of information of an alert should be done by detection engineers. Incident response automation should focus on response and remediation.\nThere\u0026rsquo;s a term I use with alert analysis that I call \u0026ldquo;friction\u0026rdquo;. Friction is anything that slows down analysis in an unhelpful way. For example, having to go to another system to look something up, not knowing what playbook to use, a swarm of false positives, etc. All of these add friction to the alert triage process slowing it down and increasing the possibility of errors. As a threat detection engineer, you should strive to reduce friction when designing your alerts.\nSo what does a good alert look like?\nTitle: Phishing Reported by User User: john.doe@snowflake.com Details: The attached email was reported as phishing by the user. The email has been removed from the user's inboxes. Similar emails were found in the following user's inboxes and have been temporarily quarantined: jane.doe@snowflake.com, bob.human@snowflake.com, lee.user@snowflake.com. Information on the attachment and URLs in the email is attached to the ticket. Timestamps: 14:33 UTC Playbook: Review the attached phishing emails and analysis. To remove the similar emails from the associated user's inboxes, click here \u0026lt;link\u0026gt;. To release from quarantine back into the users' inbox, click here \u0026lt;link\u0026gt;. Attachments: \u0026lt;simulated analysis\u0026gt; This alert does a few things right: first, it has automated analysis of the phishing email and attached it to the ticket. This reduces the friction an analyst experiences when working a ticket. Second, it\u0026rsquo;s clear what has already been done: the reported email has been removed from the user\u0026rsquo;s inbox. Third, it presents a call to action: the system has identified similar emails and requires human analysis to determine what the next steps should be. Finally, it makes it easy to take the next steps by presenting links that trigger a playbook. When designing alerts, make sure to consider that you should reduce analysis friction and make it easy to take the next steps.\nAutomatically Enriched This property is highly related to the one above, and is arguably a subset; however, I believe it\u0026rsquo;s important enough where I\u0026rsquo;ve decided to specifically call it out. This property can be summed up succinctly by answering the following question: Does the human analyst need to go into another system to gather additional information? If the answer is yes, you can improve the alert.\nWith that in mind, let\u0026rsquo;s revisit the example above.\nTitle: Phishing Reported by User User: john.doe@snowflake.com Timestamps: 14:33 UTC When an analyst gets an alert about phishing, it\u0026rsquo;s your job as a detection engineer to understand what questions the SOC/IR will have and work to answer as many of those in the alert as you can. (The best way to know what questions they\u0026rsquo;ll ask is to go talk to them.) In this case, some common questions when triaging phishing are:\n Did the user engage with the phish? Did they respond, click a link, or download anything? Who else was targeted? What do we know about the phishing links/attachments/campaign?  By knowing what questions the analyst will have, we can enrich our alerts (with whatever SIEM/SOAR you\u0026rsquo;re using) to reduce analyst friction.\nWell Prioritized Doing good work is easy. You just have to do the right things in the right order. With the previous example, we\u0026rsquo;ve helped the SOC/IR do the right things by providing them the tools they need to execute. The next part we need to help with is alert prioritization. Most SOCs have some sort of SLA when it comes to alerts that varies by severity. A basic example could be\n    Critical High Medium Low     SLA Pageable 1 hour 12 hours 24 hours    If you write the most accurate alert for someone running whoami on a laptop and you set it as a critical paging your IR at 3 AM, you\u0026rsquo;re going to have some very unhappy responders. As a detection engineer, you can\u0026rsquo;t forget that you have a customer and that\u0026rsquo;s the individuals responding to your alerts. It is of utmost importance that you equip them with the information they need to make the correct decisions quickly. And that means helping them prioritize their work.\nWhen it comes to severity, I\u0026rsquo;d recommend that your team decide on a framework on how it\u0026rsquo;s set. The goal is that if two different engineers write the same rule, they would end up with the same severity. Some guiding questions could be:\n Does this signal mean customer data has been compromised? What is our confidence that this alert is always going to be a true positive? Does this signal affect the integrity/availability of administrative credentials/permissions?  Ideally, you want to keep your framework to a limited number of questions. You don\u0026rsquo;t want your detection engineers to feel like they\u0026rsquo;re going through an audit every time severity is being set.\nGrouped and Correlated One of the last properties that I think is critical for writing good alerts is grouping and correlation. I\u0026rsquo;ll also associate deduplication with this category. Let me start by defining what I mean by these properties:\n Grouping/correlation: the ability to determine that two separate alerts are related Deduplication: the ability to determine that two or more separate alerts are for the same event and to group them into one  Let\u0026rsquo;s start off with deduplication by providing some examples. Let\u0026rsquo;s say we have an alert for the root user in an AWS account having a failed login. In this example, a penetration test is running and attempts to brute force the account 10,000 times. Without deduplication, IR would receive 10,000 separate tickets/cases. However, if we\u0026rsquo;re doing alert deduplication properly, they would receive 1 ticket with information that the alert triggered 10,000 times.\nDepending on your SIEM, it is often the responsibility of the detection engineer to decide how alerts are deduplicated. For instance, Panther offers deduplication as part of its Python functionality. As a detection engineer, you can solve for deduplication by asking yourself the following questions: what key properties identify a unique instance of this event? What do I want to show IR if the same event fires multiple times? At Snowflake, we often structure our alerts as \u0026ldquo;Actor\u0026rdquo; did \u0026ldquo;Action\u0026rdquo; to \u0026ldquo;Object\u0026rdquo;. Our default deduplication says if the \u0026ldquo;actor\u0026rdquo; and \u0026ldquo;action\u0026rdquo; or \u0026ldquo;actor\u0026rdquo; and \u0026ldquo;object\u0026rdquo; are the same for a particular alert, then deduplicate. To illustrate with an example, let\u0026rsquo;s say we have an alert for an action having permission denied in AWS. We say our \u0026ldquo;actor\u0026rdquo; is the IAM user, the \u0026ldquo;action\u0026rdquo; is the AWS API, and the \u0026ldquo;object\u0026rdquo; is the request parameters. Let\u0026rsquo;s say a particular user triggers the alert on \u0026ldquo;DeleteObject\u0026rdquo; 10,000 times across 10,000 different buckets. Because the user and API action are the same, these would be deduplicated and the 10,000 events would be presented in one ticket. Similarly, if a user did a number of reconnaissance actions (let\u0026rsquo;s say GetObject, GetObjectACL, etc) against a particular bucket, these would also be grouped. That being said, as a detection engineer, you should feel free to be flexible with deduplication and choose the properties that work best for your detection.\nA similar property is alert grouping/correlation. Let\u0026rsquo;s revisit the above example about permission denied for an AWS user. Let\u0026rsquo;s say we also have a detection for multiple failed logins for an AWS user. In this example, both of these alerts trigger about 3 hours apart (with permission denied occurring second) in the middle of changeover for our SOC. Without alert correlation/grouping, the analyst working the second alert may be unaware that there was previously an alert for multiple failed logins for the same user. However, if our system is able to group these alerts and show them together, then the analyst has the context they need to make smart response decisions. This capability is also a critical first step to successfully implementing Risk Based Alerting for your SOC.\nMaking it Happen Much of what I discussed in this article centers on correlation and enrichment. At Snowflake, we accomplish this through our use of Snowflake as a security data lake. By ingesting all of our enterprise data into Snowflake, we\u0026rsquo;re able to execute complex correlation and enrichment queries as part of our detection pipeline. Haider and I did a talk on this and the detection lifecylce in our talk Unlocking the Magic to High-Fidelity Alerts if you want to learn more. For the areas where we can\u0026rsquo;t accomplish this for one reason or other, we rely on Tines to execute further automation.\nConclusion I think this piece can best be summarized by remembering that if alerts are not properly triaged, then the detections are not effective. And you, as a detection engineer, have many tools to empower analysts to make the timely and correct decisions.\n",
    "ref": "/blog/howtowriteanactionablealert/"
  },{
    "title": "Real Time vs Scheduled Query Detections - A Guide For Detection Engineers",
    "date": "",
    "description": "Many SIEM tools nowadays offer the opportunity for you to write rules on streaming data or run scheduled queries on a periodic basis. But when should you use which and why? This blog post is designed to serve as a guide to those designing their detection architecture.",
    "body": "Most modern SIEMs offer 2 primary methods for running their queries: real time rules and scheduled queries. Each option offers a variety of pros and cons that you should consider as you develop your detection. Before we dive into that, let\u0026rsquo;s clarify what we mean by each.\nDefinitions A real time rule, often called a streaming rule, runs on a stream of data. Essentially, as the data enters the SIEM, each data point will get processed against the rules that correspond with that kind of data. It\u0026rsquo;s important to note that these are actually more accurately \u0026ldquo;near\u0026rdquo; real time rules. There\u0026rsquo;s delays due to log generation, ingestion, parsing, running the rule logic, and delivering the results. Let\u0026rsquo;s take a look at some examples.\nOKTA_SUPPORT_ACCESS_EVENTS = [ \u0026quot;user.session.impersonation.grant\u0026quot;, \u0026quot;user.session.impersonation.initiate\u0026quot;, ] def rule(event): return event.get(\u0026quot;eventType\u0026quot;) in OKTA_SUPPORT_ACCESS_EVENTS def title(event): return f\u0026quot;Okta Support Access Granted by {event.udm('actor_user')}\u0026quot; def alert_context(event): context = { \u0026quot;user\u0026quot;: event.udm(\u0026quot;actor_user\u0026quot;), \u0026quot;ip\u0026quot;: event.udm(\u0026quot;source_ip\u0026quot;), \u0026quot;event\u0026quot;: event.get(\u0026quot;eventType\u0026quot;), } return context The above rule is from Panther and can be found at okta account support access.\nThe rule is configured to run on Okta logs. For every event, it determines if the event type was in a list of defined Okta support access events. This rule runs as the Okta logs are ingested into Panther and is near-real time: as soon as the logs are shipped to Panther, the rule runs.\nNow, let\u0026rsquo;s take a look at a scheduled query.\nAnalysisType: scheduled_query QueryName: Okta Investigate MFA and Password resets Enabled: true Description: \u0026gt; Investigate Password and MFA resets for the last 1 hour SnowflakeQuery: \u0026gt; SELECT p_event_time, actor:alternateId as actor_user, target[0]:alternateId as target_user, eventType,client:ipAddress as ip_address FROM panther_logs.public.okta_systemlog WHERE eventType IN ('user.mfa.factor.reset_all', 'user.mfa.factor.deactivate', 'user.mfa.factor.suspend', 'user.account.reset_password', 'user.account.update_password', 'user.mfa.factor.update') and p_occurs_since('1 hour') ORDER by p_event_time DESC Schedule: RateMinutes: 60 TimeoutMinutes: 1 The above query can be found here: okta mfa reset (though it was modified for this blog post)\nThis query operates on data that has already been ingested into the SIEM. Every hour, it runs a SQL query over the Okta system logs and looks for events related to MFA. If any match, it will generate an alert.\nTypes of Cost I believe that nearly any rule you write as a streaming rule you can also write as a scheduled query and vice versa. So why would one choose one over the other? It comes down to cost in more ways than one. First, let\u0026rsquo;s identify the types of cost.\n Real Time Analytics - some SIEMs charge for the cost of running real time analytics. Batch Processing - some SIEMs will charge for running analytics and may have different charges for real time vs scheduled rules. Enrichment - nearly every alert nowadays requires enrichment. With a real time rule, this often can be done with lookup tables or calling an external API. With a scheduled query, one can use joins. Development - time is money friends! If a rule takes longer to develop and test, that\u0026rsquo;s a cost that needs to be accounted for. Error Potential - failures can be disastrous for a SOC as they can cause false negatives leading to an extended breach. What can cause errors for these rules? For a streaming rule relying on an API for enrichment, you can run into rate limiting or just general API failures if the API goes down. For a scheduled query where you\u0026rsquo;re relying on joins, you can run into data ingestion delay sync issues. Let\u0026rsquo;s dive into this more.  Data Ingestion Failures Let\u0026rsquo;s refer to our above scheduled query. In this scenario, we\u0026rsquo;re ingesting Cloudtrail, Okta, and Jamf into our data lake. Let\u0026rsquo;s imagine that our Jamf ingestion gets delayed for whatever reason. And during this delay, a user starts working from a coffee shop with a new public IP address. We could very easily run into a false positive as the user\u0026rsquo;s new laptop IP would not be in our data lake yet. We can also imagine similar false negatives where an inner join can fail due to data ingestion. How do we solve this issue? We have a couple of options.\n Staggered data lookbacks. Instead of looking back from -1 hour to present, we could look back -2 hours to -1 hours. This gives us a buffer window for data to hit our data lake. However, it automatically increases our time to detect by 1 hour. Overlapping lookbacks. Now let\u0026rsquo;s imagine we lookback -2 hour to present but we run every 1 hour. This keeps our time to detect at a more reasonable threshold, but we\u0026rsquo;re greatly increasing the volume of data we\u0026rsquo;re examining. We\u0026rsquo;ll also have to be cautious of how we plan for alert deduplication.  Example Costs Let\u0026rsquo;s take a look at some examples of common detections and see where their costs are.\n Streaming rule on firewall logs looking for IoC IPs  Let\u0026rsquo;s look at some example code.\nfrom ioc_library import lookup_ioc def rule(event): return lookup_ioc(event.udm(\u0026quot;source_ip\u0026quot;)) or lookup_ioc(event.udm(\u0026quot;dest_ip\u0026quot;)) def title(event): if lookup_ioc(event.udm(\u0026quot;source_ip\u0026quot;)): return f\u0026quot;Source IP ({event.udm('source_ip')}) matched a known IoC\u0026quot; else: return f\u0026quot;Dest IP ({event.udm('dest_ip')}) matched a known IoC\u0026quot; In this sample code, we have a library that allows us to pass in an IP and query an API to see if the IP matches a known IoC. What potential costs could we have?\nWe\u0026rsquo;re going to have real time analytic and enrichment costs. Especially concerning is the fact that we\u0026rsquo;re calling the lookup_ioc function up to 3 times. (Note that this is a naive implementation to serve as an example.) Depending on the volume of our firewall traffic, it\u0026rsquo;s very likely we could run into a rate limit on the API.\nScheduled query matching AWS logins with Okta usernames with Jamf IPs  AnalysisType: scheduled_query QueryName: Okta AWS Console Logins from Non-Laptops Enabled: true Description: \u0026gt; Investigate AWS console logins from Non-Laptops SnowflakeQuery: \u0026gt; SELECT * FROM panther_logs.public.cloudtrail ct INNER JOIN panther_logs.public.okta_users okta on okta:email like CONCAT('%', lower(ct:userIdentity.userName), '%') LEFT OUTER JOIN panther_logs.public.jamf_inventory jamf on jamf:externalIP = ct:sourceIPAddress WHERE ct:eventName = 'ConsoleLogin' and p_occurs_since('1 hour') and jamf:externalIP is NULL ORDER by p_event_time DESC Schedule: RateMinutes: 60 TimeoutMinutes: 1 In the above login, we\u0026rsquo;re joining our Cloudtrail data with Okta, and then joining that again on Jamf data to see where users are logging in from devices that aren\u0026rsquo;t their laptops. How would we write this if it was a streaming rule? Similarly to the first example, we\u0026rsquo;d have to have an API where we could pass an IP and see if that IP is associated with one of our devices in Jamf.\nWhat are the costs here?\nWe\u0026rsquo;re going to have processing costs in our SIEM - if we have a lot of Cloudtrail logs, we could be searching a lot of data. Additionally, while not super relevant in this example, depending on the join we could face costs/performance concerns.\nDeciding on a Style Now that we\u0026rsquo;re familiar with some of the costs, what questions should we ask ourselves when designing a rule?\nFactors that favor real time rules How quickly do I need to be alerted?\nA streaming rule will notify your SOC/IR team in near-real time. If the rule doesn\u0026rsquo;t require an immediate response, one could use a scheduled query.\nHow much data do I need to look at?\nLet\u0026rsquo;s envision we\u0026rsquo;re using a scheduled query that\u0026rsquo;s using data source A and B, and we\u0026rsquo;re joining the past hour of A on all of B. If B is a very voluminous data source, we could be looking at a very slow query. In this case, it may be better to run a streaming rule on A and then query B as necessary in the next step of the pipeline.\nHow many rules do I have running on this data set and how large is the data set?\nLet\u0026rsquo;s say we\u0026rsquo;re writing rules on VPC flow log data which has a LOT of data. Let\u0026rsquo;s say we want to write 5 different rules to look for IoCs, unexpected ports, etc. (ones that are very simple). If we were to write 5 different scheduled queries, we would be looking at the same (large amount of) data multiple times. This can get expensive. We could write 1 scheduled query but it would be complicated and hard to parse. So in this case, writing streaming rules would make more sense and we could rely on downstream enrichments.\nFactors that favor scheduled queries Do I need enrichment to generate an alert?\nIn our first example, we needed to enrich our data in order to generate an alert. This means we\u0026rsquo;ll need to call an API on every data record. On the other hand, if we only need to call the API after we\u0026rsquo;ve decided to generate an alert, we\u0026rsquo;ll significantly reduce how often we\u0026rsquo;ll need to call the API and avoid potential rate limit concerns.\nHow many data points do I need to make a conclusion?\nIn our second example, we needed Cloudtrail, Okta, and Jamf in order to make a determination if the login was malicious. This would be difficult to do with a real time rule as it would likely require the development of specific infrastructure to support these lookups.\nHow well does my infrastructure handle failure?\nLet\u0026rsquo;s say you hit a rate limit and your API lookup fails. What happens? Does your infrastructure rerun the rule at a later date? Or does your infrastructure ignore the event and not process it? If you suspect you may hit a rate limit with your rule and your infrastructure fails silently, you should consider re-designing the rule or running it as a scheduled query.\nDo I need to consider system state?\nLet\u0026rsquo;s imagine a scenario where we have an endpoint security tool, and we want to alert if it\u0026rsquo;s uninstalled. However, our IT team has a script that is triggering false positives because it temporarily disables the agent when it applies updates. So we want to know what state the system was in when the disable command was run. You could potentially set up some sort of state analysis pipeline, but it can get prohibitively expensive. It\u0026rsquo;s much simpler to use joins or something like match_recognize.\nOther Factors What\u0026rsquo;s my team familiar with?\nIf your team is more familiar with real time rules, you should defer to that. It\u0026rsquo;s likely your existing infrastructure and tooling better supports development and testing of those rules. That doesn\u0026rsquo;t mean you should ignore the alternative all together, though!\nWhat infrastructure and supporting automation and tooling have we already built?\nAre you architected around ingesting all your data into your security data lake? Or do you have a robust series of APIs to query 3rd party endpoints? This could drive your decision one way or the other; though with Snowflake\u0026rsquo;s UDF support, you could call these APIs from your scheduled queries if you wish.\nConclusion There\u0026rsquo;s some great blog posts on detection engineering out there, but I often see them focusing too much on theory or on the nitty gritty of some technology. My hope is that this article will be useful to practioners especially those who are charged with architecting their detection systems.\nNow that you’ve got this information and are equipped with these questions, you’re prepared to make better decisions when it comes to designing your rules. Are there other things you consider during your design process? Please let me know!\n",
    "ref": "/blog/streamingvsscheduling/"
  },{
    "title": "A Shmoo's Guide to DC",
    "date": "",
    "description": "While the hotel bar and the restaurants right around it present an easy opportunity to fill up, I feel like I owe it to provide some recommendations of where I think some of the best places to hang out around the hotel are.",
    "body": "Some of these recommendtions work best for Shmoocon in March vs Shmoocon in the cold of January.\nOrienting Yourself\nBelow is a map of the neighborhoods. The gold star is the Hilton hotel. When you hear someone reference a neighborhood, hopefully this helps. The most common ones you\u0026rsquo;ll probably hear are Adams Morgan (Admo), Dupont Circle (Dupont), U St, The Mall, and Capitol Hill.\nGetting Around\nMetro\n Dupont station is going to be closest and that\u0026rsquo;ll get you to the red line. You probably won\u0026rsquo;t need to take the Metro unless you\u0026rsquo;re heading to Amtrak or heading out to Virginia or DCA (Airport).   Buses\n Metrobus. This is the main bus system for DC. It\u0026rsquo;s heavily commuter focused. https://www.wmata.com/schedules/maps/. Circulator  https://www.dccirculator.com/ride/rider-tools/interactive-map/ Your best bet to get to georgetown without walking. It comes frequently and it\u0026rsquo;s only $1.    Hopping Areas\n Admo Dupont 14th St NW (between Logan Circle and U St)  Barcelona  Wine bar   Player’s Club  Arcade bar   Chicken \u0026amp; Whiskey Colada Shop  Cuban \u0026amp; Cocktails     U St  Head towards the loud music near 801 Florida Ave NW, Washington, DC 20001 You can’t really mess up just bar hoping hear Lots of rooftop areas   Georgetown  I don’t go out hear a lot, but there’s some decent bars. You’re going to get a different group here - mix of college students and older DC residents   The Navy Yard  It\u0026rsquo;s a bit of a hike, but worth it imo. Good restaurants, bars, and you\u0026rsquo;re right on the water. Good for a nice Spring day.    Drinking\nNearby\n McClellans Reterat Board Room  Board games!   Dan’s Cafe  Notorious for their pour your own drinks   Jack Rose  Whiskey    Cocktails\n Barmini - expensive, reservation required, but so good Thai Chef Street Food - surprisingly good cocktails Residents dC - https://www.residentsdc.com/menus/#cocktail-menu thanks @jamieantisocial Anyone have other recommendations for ones near the hotel?  Whiskey\n Jack Rose  Speakeasies\n Chicken \u0026amp; Whiskey (in the back behind the metal door) Left Door (Need a reservation I believe) The Gibson (Need a reservation I believe)  Partying\n U st (search around TAKODA)  Eating\nNearby\n Lucky Buns (Long wait) Thai Chef Street Food  My favorite Thai in the city Great cocktails    Pizza\n Andys  Thai\n Thai Chef Street Food (You maybe can tell I like this place)  Steak\n Sourcing recommendations  Ethiopian\n DC is famous for their ethiopian, but I actually haven\u0026rsquo;t had it recently, so I\u0026rsquo;m sourcing recommendations  Late bites (nearby)\n Dc pizza \u0026amp; kabob Duccinis El Tamarindo McDonalds (Admo) Andys (Admo)  Snacks/Sweets\n Foxtrot (like an upscale 7-11) Insomnia cookies - late night cookies Jenis - my favorite ice cream place Un je ne sais Quoi… - really good french bakery  Breakfast\n Bethesda bagel Ted’s Bulletin - alcoholic milkshakes and the homemade poptarts are the stars  Just Plain Good\n Anju Iron Gate Jinya (ramen)  Chinese\n Great Wall Szechuan. It’s the best. I won\u0026rsquo;t be debating this :)  Other\n Dupont farmers market  Call your mother bagels - best bagels in DC Fresh fruit    Outdoors\n Meridian Hill Park. This is a really nice park nearby. When the weather is nice, there\u0026rsquo;s a lot of people hanging out, dogs, music, etc. National Mall. Of course. Do your best to skip the food trucks. They\u0026rsquo;re generally not worth it.  Restroom map: https://www.nps.gov/common/uploads/grid_builder/nationalmall4th/crop1_1/653C856A-C02B-92D6-BD93E5C8A469CDBC.jpg   Rock Creek Park. This park can take you pretty far. https://www.alltrails.com/explore/us/washington-dc/washington?b_tl_lat=38.925643460593164\u0026amp;b_tl_lng=-77.0563698317884\u0026amp;b_br_lat=38.91909424386935\u0026amp;b_br_lng=-77.04043356221366  ",
    "ref": "/blog/ashmoosguidetodc/"
  },{
    "title": "Azure Flow Log Analysis",
    "date": "",
    "description": "Azure flow logs don't have the same instance ID that AWS flow logs do. So how do you figure out which VM the logs came from?",
    "body": "Intro Disclaimer I currently work at Snowflake and use the product on a daily basis for log analysis and threat detection. At the time of this writing, that probably adds bias to my article.\nAt Snowflake, we\u0026rsquo;re a multi-cloud environment. As part of the threat detection team, it\u0026rsquo;s my job to ensure that we\u0026rsquo;re monitoring. One of my tasks was to monitor our Azure flow logs for unusual behavior. Information on Azure Network Security Group (NSG) flow logs can be found at this link. When I started exploring this data source, I was frustrated to learn that, unlike the AWS VPC flow logs, there was no field that ties the flow log to the instance ID, or in the case of Azure, the Virtual Machine. The recommended solution from Microsoft was to use the IP address. However, the architecture I was working in had thousands of worker nodes created and destroyed frequently meaning that we would often see duplicate IP addresses within a short period of time due to NATing. If I couldn\u0026rsquo;t automatically tie the IP to a VM, the logs would be useless as our IR team needed that information in order to conduct automated response in case of a malicious event.\nPlotting Fortunately, I had SQL to help. To conduct my analysis, I used Snowflake, which is where we (obviously) store our logs. However, any SQL tool should offer the same capabilities below. In order to conduct this analysis, I used a trick taught to me by one of the data scientists at Snowflake. First, for Azure logs, IPs aren\u0026rsquo;t associated with VMs. They\u0026rsquo;re associated with NICs and a NIC is associated with a VM. Now, a lot of organizations use various tools and scripts that collect information about current assets periodically. However, considering Snowflake creates and destroys VMs and NICs so rapidly, this isn\u0026rsquo;t sufficient. We also needed to look at the Azure operation logs in order to establish confidence that we were looking at every NIC. Let\u0026rsquo;s take a look at some code.\nselect parse_json(propertiesresponseBody) as response_body, response_body:etag::string as etag, response_body:name::string as nic_name, response_body:location::string as nic_locatin, response_body:properties as nic_properties, response_body:tags as tags, response_body:type::string as nic_type, value:properties.privateIPAddress::string as nic_ip from azure_operation_logs_v, lateral flatten(parse_json(properties:responseBody):properties.ipConfigurations) where operation_name = 'MICROSOFT.NETWORK/NETWORKINTERFACES/WRITE' You can review the documentation on the structure here: Network Interfaces - Create Or Update.\nThis gets us information on NICs from our operation logs. We also had have scripts running that collect information on existing NICs. This is useful for looking at longer running VMs.\nwith nic_collect_logs as ( select id::string as nic_id, etag::string as etag, nic_name::string as nic_name, nic_location::string as nic_location, nic_properties, tags, nic_type::string as nic_type, value:properties.privateIPAddress::string as nic_ip 'collect_script' as source min(recorded_at) as earliest, max(recorded_at) as latest from AZURE_COLLECT_NETWORK_INTERFACES_V, lateral flatten(properties:ipConfigurations) group by 1,2,3,4,5,6,7,8,9 ) select * from nic_collect_logs; This is slightly different. We wanted to remove duplicate NICs by using group by and also note the first time we saw the NIC and the most recent time we\u0026rsquo;ve seen the NIC. Let\u0026rsquo;s apply the same to the NICs collected by the operation logs.\nwith nic_create_op_logs_raw as ( select parse_json(propertiesresponseBody) as response_body, response_body:etag::string as etag, response_body:name::string as nic_name, response_body:location::string as nic_location, response_body:properties as nic_properties, response_body:tags as tags, response_body:type::string as nic_type, value:properties.privateIPAddress::string as nic_ip value:properties.entity::string as nic_id, 'operation_logs' as source from azure_operation_logs_v, lateral flatten(parse_json(properties:responseBody):properties.ipConfigurations) where operation_name = 'MICROSOFT.NETWORK/NETWORKINTERFACES/WRITE' ), nic_create_op_logs as ( select nic_id, etag, nic_name, nic_location, nic_properties, tags, nic_type, nic_ip, source min(event_time) as earliest, max(event_time) as latest from nic_create_op_logs from nic_create_op_logs group by 1,2,3,4,5,6,7,8,9 ) select * from nic_create_op_logs; So now we have NICs created by operational logs and NICs identified from our collection scripts. Let\u0026rsquo;s combine them together using union.\nwith all_nics as ( select * from nic_create_op_logs UNION select * from nic_collect_logs ) select * from all_nics; Now, the issue comes up where we want to know the window of time that a NIC had a particular IP address. We can do this using the lead and lag functions to get the start and end window times for a NIC\u0026rsquo;s association with an IP address. In this case, we can use true_earliest as the window start. To find the window_end, we\u0026rsquo;ll use lead\nselect *, true_earliest as window_start, lead(true_earliest) over ( partition by nic_ip order by true_earliest asc ) as window_end from all_nics ; The Code Putting all the code together we have\nwith nic_collect_logs as ( select id::string as nic_id, etag::string as etag, nic_name::string as nic_name, nic_location::string as nic_location, nic_properties, tags, nic_type::string as nic_type, value:properties.privateIPAddress::string as nic_ip 'collect_script' as source min(recorded_at) as earliest, max(recorded_at) as latest from AZURE_COLLECT_NETWORK_INTERFACES_V, lateral flatten(properties:ipConfigurations) group by 1,2,3,4,5,6,7,8,9 ), nic_create_op_logs_raw as ( select parse_json(propertiesresponseBody) as response_body, response_body:etag::string as etag, response_body:name::string as nic_name, response_body:location::string as nic_location, response_body:properties as nic_properties, response_body:tags as tags, response_body:type::string as nic_type, value:properties.privateIPAddress::string as nic_ip value:properties.entity::string as nic_id, 'operation_logs' as source from azure_operation_logs_v, lateral flatten(parse_json(properties:responseBody):properties.ipConfigurations) where operation_name = 'MICROSOFT.NETWORK/NETWORKINTERFACES/WRITE' ), nic_create_op_logs as ( select nic_id, etag, nic_name, nic_location, nic_properties, tags, nic_type, nic_ip, source min(event_time) as earliest, max(event_time) as latest from nic_create_op_logs from nic_create_op_logs group by 1,2,3,4,5,6,7,8,9 ), with all_nics as ( select * from nic_create_op_logs UNION select * from nic_collect_logs ), select *, true_earliest as window_start, lead(true_earliest) over ( partition by nic_ip order by true_earliest asc ) as window_end from all_nics; And now we have NICs and the IPs associated with them and the window of time when that IP was associated with that NIC.\nOur next step is to join this on VM data. That\u0026rsquo;s fairly easy.\nFor our Azure VMs, we did a similar exercise of joining data from Collect script and Operational logs. We\u0026rsquo;ll call this view AZURE_VMS_V. We\u0026rsquo;ll call our Azure NICs view AZURE_NICS_V.\nselect * from AZURE_NICS_V a join AZURE_VMS_V b on lower(a.id) = lower(b.properties['networkProfile']['networkInterfaces'][0]['id]) A few notes - I used lower since there were some instances where the ID had varying cases and other instances where it didn\u0026rsquo;t. I\u0026rsquo;m attributing this to inconsistences with the APIs between collection scripts vs operational logs. You may also note that I used 0 instead of lateral flatten for accessing the network interfaces of the VMs. In our environment, we do not have VMs where there are multiple NICs. If you do, you can use lateral flatten again.\nThe final step is to join our flow logs on our NICs. In this way we\u0026rsquo;ll have joined Flow Log \u0026lt;\u0026gt; NIC \u0026lt;\u0026gt; VM. Fortunately, most of the hardwork is done.\nselect * from AZURE_FLOW_LOGS_V left outer join AZURE_NICS_V on src_addr = nic_ip and event_time \u0026gt;= window_start and (event_time \u0026lt;= window_end or window_end is NULL) left outer join AZURE_VMS_V on lower(AZURE_NICS_V.id) = lower(AZURE_VMS_V.properties['networkProfile']['networkInterfaces'][0]['id]) For joining the flow logs on the NIC, we want to ensure that the Flow Log event time occurred after the window start for the NIC and before the end time of the NIC. If the NIC still exists, the window_end would be NULL.\nIf you seek higher fidelity, you can also look at including tenant ID and subscription ID into your joins.\nThe obvious question is \u0026ldquo;how accurate is this?\u0026rdquo;\nLet\u0026rsquo;s check. I\u0026rsquo;ll call our above logic AZURE_FLOW_LOGS_JOIN_VMS_V. We\u0026rsquo;ll look for null VM IDs (where we couldn\u0026rsquo;t join on a VM) and the flow was recorded in the past day. (flow_recorded_at is a column we add during ingestion that records the time the flow was ingested into Snowflake.)\nselect count(*) from AZURE_FLOW_LOGS_JOIN_VMS_V where vm_id is NULL and flow_recorded_at \u0026gt;= dateadd(hour, -1, current_timestamp()) and something similar for not null showed me that 99.75% of flow logs could be matched to their VM.\nConclusion This was a lot of work for something that AWS offers natively. I hope that as Azure matures their product that they\u0026rsquo;ll also offer similar levels of visibility.\nYou may be wondering why I used IPs instead of MAC addresses. When inspecting our logs, a vast number of NICs has IPs but no MAC addresses. I do not have a clear reason from Microsoft why that is at the time of this publication.\nI hope this work helps those working in security and Azure clouds. If you have other ideas or ways to improve the success rate, please let me know!\n",
    "ref": "/blog/azureflowloganalysis/"
  },{
    "title": "About",
    "date": "",
    "description": "Daniel's thoughts on infosec",
    "body": "I\u0026rsquo;m a security engineer at Snowflake who also enjoys computers, coffee, cats, Muay Thai, and BJJ\n",
    "ref": "/about/"
  },{
    "title": "Regarding SMS 2FA",
    "date": "",
    "description": "Responding to Tavis Ormandy's comments on SMS 2FA",
    "body": "I think an important question that Tavis either explicitly or accidentally omitted is \u0026ldquo;for whom\u0026rdquo;. I am not sure why he did not include this as it\u0026rsquo;s a critical component to his argument. If Tavis is stating that \u0026ldquo;SMS 2FA is ineffective for an enterprise\u0026rdquo;, then I would agree. The threat model that he is operating from is that an organization is being explicitly targeted by a motivated (though not necessarily extremely capable) attacker, who only needs minimal access to cause great harm. However, if Tavis is instead simply stating \u0026ldquo;SMS 2FA is ineffective\u0026rdquo; with no caveats, I would disagree.\nWhen considering SMS 2FA, one needs to look at their threat model and personal risk. To argue this point, I will attempt to address his points from the perspective of a technologically immature individual looking to secure their bank account. In this context, one\u0026rsquo;s threat model changes from \u0026ldquo;I need to protect all the accounts\u0026rdquo; to \u0026ldquo;I simply need to make sure my account is more secure than someone else\u0026rsquo;s\u0026rdquo;. From this perspective, SMS 2FA provides significantly more protection from credential stuffing than a username/password combination. In addition, the argument of \u0026ldquo;an attacker can simply move to a different service\u0026rdquo; no longer holds as not all accounts for this user are of equal importance. Access to their neopets account is significantly less valuable than their bank account.\nAnother one of Tavis\u0026rsquo; arguments is \u0026ldquo;Instead, why not simply randomly generate a good password for them, and instruct them to write it down or save it in their web browser? If they lose it, they can use your existing password reset procedure.\u0026rdquo; I think this argument ignores the reality of today\u0026rsquo;s world in that, if a commercial service forced that kind of policy, it would not be adopted by users. Anyone who has worked with a technologically immature user can attest to how common it is for users to forget or lose their password. While, again, I can see such a service working for an enterprise product, I do not see such an authentication scheme being acceptable for a B2C product.\nI\u0026rsquo;d also like to discuss his point of \u0026ldquo;We have a finite pool of good will with which we can advocate for the implementation of new security technologies. If we spend all that good will on irritating attackers, then by the time we’re ready to actually implement a solution, developers are not going to be interested.\u0026rdquo; As far as I can tell, Tavis is arguing that if one invests their time in implementing 2FA, then developers will not be interested in allowing or enforcing more secure options. Coming from the perspective of someone who has worked with numerous developers during my time with consulting, I do not feel that this is true. Security is top of mind for numerous companies. Account compromise not only has a cost to a user, but also has a cost the service provider. This can be in terms of refunds, time spent restoring access, or other direct resource cost. Additionally, I think Tavis is oversimplifying the argument. His statement of \u0026ldquo;all that good will\u0026rdquo; implies that implementing SMS 2FA will \u0026ldquo;use up\u0026rdquo; security\u0026rsquo;s ability to influence the security features a product will implement. He exhibits a false dichotomy logical fallacy where he represents the decision as either all or nothing.\nSecurity is ultimately a risk calculation. SMS 2FA doesn\u0026rsquo;t prevent malware and it doesn\u0026rsquo;t prevent phishing. However, risk is about using your resources to make the impact or likelihood of a failure event. SMS 2FA does just that by efficiently reducing the likelihood of credential stuffing attacks. Do I wish that all sites supported U2F for 2FA and do I encourage everyone to use a password manager to generate random passwords? Yes. Will I continue to recommend that enterprises not support SMS 2FA for SSO and other solutions? Yes. However, I recognize the reality that we live in that password-based authentication is a bandaid for the larger identity problem for the world wide web and B2C services. And I will continue to tell my friends and family that SMS 2FA is better than nothing. If you have counterarguments, please feel free to reach out via email or twitter, and I\u0026rsquo;d be happy to respond in an addendum to this post. Have a great weekend and remember to be respectful in your discourse.\n",
    "ref": "/blog/regardingsms2fa/"
  },{
    "title": "Adding to the Dialogue - On the Release of Offensive Security Tools (OST)",
    "date": "",
    "description": "After a lot of dialogue recently on the release of Offensive Security Tools, I thought I would add to the dialogue in a more long-form format.",
    "body": "Update 1 - I\u0026rsquo;m clarifying the definition of Advanced Persistent Threats (APTs) and Financially Motivated Actors (FMAs). I combined the two groups in the previous version. The content and focus of the discussion primarily centers on FMAs. APTs and FMAs can overlap in terms of TTPs, capabilities, personnel, countries, etc. What distinguishes them is motivation. FMAs, as their name implies, are financially motivated. APTs can have a number of motivations including financial, political, etc.\nTwitter is a complicated social networking platform. Then again, which platform isn\u0026rsquo;t? I\u0026rsquo;m fairly new to tweeting, but I\u0026rsquo;ve already found that it can be a tremendous resource when it comes to receiving up-to-date information on new techniques, upcoming talks, or nifty vulnerabilities. What I haven\u0026rsquo;t found it useful for is having discussions, especially those surrounding a controversial idea. This has manifested in my feed over the past month with the discussion around the release of Offensive Security Tools, primarily ignited by Andrew Thompson (@QW5kcmV3).\nhttps://twitter.com/ItsReallyNick/status/1171145995050205185/photo/1\nOffensive Security Tools Before you read anymore, please read Andrew\u0026rsquo;s blog post on the Unrestricted Release of Offensive Security Tools as I\u0026rsquo;ll be using verbiage defined in that article. (Based on some of the discussions on Twitter where people have straw manned Microsoft\u0026rsquo;s operating system as an Offensive Security Tool (OST), I have confidence that not many people have actually read the article.) Andrew\u0026rsquo;s primary argument is that the unrestricted release of OST is causing more harm than good to the world. He\u0026rsquo;s passionately championed this idea on Twitter, pointing to data such as that available from his employer (FireEye/Mandiant) about actual breaches. Reaction to his argument has ranged from agreement to vitriolic outrage. (I do concede that some of the negative feedback may be due to his tone, which has not always been the friendliest.) Opposition has accused him of gatekeeping, exaggerating the magnitude of the problem, or just being plain wrong. Others have argued that this is a solved problem and wonder why it\u0026rsquo;s even being discussed. Later in this article, I will attempt to address and refute some of those arguments.\nSafe Spaces The second article I\u0026rsquo;d like you to read is, on the surface, unrelated to the discussion at hand. It\u0026rsquo;s a recently published piece by James Hatch titled My semester with the snowflakes. In the article, James discusses some of the assumptions he made about the students of Yale University prior to his first semester at the institution and how these assumptions were summarily shattered. One of these assumptions regarded the term \u0026ldquo;safe space\u0026rdquo;. James had long regarded the term to mean a place to discuss ideas without having one\u0026rsquo;s feelings hurt. He learned the truth is quite the opposite. \u0026ldquo;What she [his fellow student meant by ‘safe space\u0026rsquo; was that she was happy to be in an environment where difficult subjects can be discussed openly, without the risk of disrespect or harsh judgement.\u0026rdquo; Throughout the rest of this post, I\u0026rsquo;d like to adopt the same definition.\nTwitter is Not a Safe Space At this point, I\u0026rsquo;m hoping you can see where I\u0026rsquo;m going with this section. The recent tone of the dialogue surrounding Andrew\u0026rsquo;s argument shows me that we are more likely moving further from a solution than towards one. Individuals on both sides of the argument have been quick to judge each other\u0026rsquo;s motives and credentials and have sometimes done so with a healthy dollop of disrespect. Twitter is not a \u0026ldquo;safe space\u0026rdquo;. Your most likely reaction to that sentence is \u0026ldquo;well, no shit\u0026rdquo;. Why, then, is so much of this discussion happening on Twitter? It clearly is not the place to discuss a topic that is clearly controversial. It is true that one can reach a global audience instantly; however, it encourages short, witty responses instead of a deep dialogue. Where then, do we turn? Conferences are another common vehicle for presenting ideas. Unfortunately, they too do not lend themselves to the idea of having a dialogue. Conferences are primarily unidirectional communication: a speaker lectures an audience. There may be time for questions, but its brevity does not encourage in depth dialogue. Additionally, conferences artificially restrict themselves to those with the resources (time, money, influence) to attend.\nAt this point in time, I am not aware of a currently existing solution for having such discussions. During my time in the Air Force, teams would sponsor \u0026ldquo;working groups\u0026rdquo; with attendees from a variety of squadrons with different perspectives to gather and discuss complicated and controversial ideas. Such a solution in the commercial space would require significant sponsorship and buy-in from employers who would lose valuable resources for a period of time as they worked on a problem affecting a community. But it\u0026rsquo;s an idea.\nGatekeeping Another term I want to define as I\u0026rsquo;ve seen it come up quite a bit is \u0026ldquo;gatekeeping\u0026rdquo;. I\u0026rsquo;ll go with the Urban Dictionary definition which is \u0026ldquo;when someone takes it upon themselves to decide who does or does not have access or rights to a community or identity.\u0026rdquo; I\u0026rsquo;ll address gatekeeping later in this article, but I wanted to clarify the definition I\u0026rsquo;ll be using.\nArguments Against the Restriction of OST Release At this point in time, I\u0026rsquo;ve seen a number of arguments against Andrew\u0026rsquo;s position. I\u0026rsquo;ll be addressing each of these in their own section.\n The problem of restricting OST is too difficult. We have already solved this problem. The problem is blown out of proportion. There are two components to this.  FMAs aren\u0026rsquo;t using public OSTs. If OST wasn\u0026rsquo;t released, FMAs would develop their own.   Restricting the release of OST will gatekeep the offensive security community.  Logical Fallacies In addressing Andrew\u0026rsquo;s arguments, I\u0026rsquo;ve seen quite a few logical fallacies. I\u0026rsquo;ll address some examples I\u0026rsquo;ve seen here, so that others can be aware. For those who have used these arguments, I would encourage you to review the list and think to yourself if you\u0026rsquo;re committing one of these fallacies when engaging in debates.\nStraw Man Fallacy Windows is used by attackers - therefore you\u0026rsquo;re proposing we restrict the release of Operating Systems. This is obviously ludicrous, therefore, the argument to restrict OST is invalid.\nHere, someone is misrepresenting Andrew\u0026rsquo;s argument by providing an easy to refute example. It is \u0026ldquo;superficially similar but ultimately not equal version of [Andrew\u0026rsquo;s] real stance\u0026rdquo;.\nBandwagon Fallacy The majority of the community thinks restricting OST is wrong. Therefore, your proposition is invalid.\nJust because the majority agrees on something, doesn\u0026rsquo;t mean it\u0026rsquo;s correct.\nThe False Dilemma Fallacy Either we restrict all tools that can be used for attackers (including Empire, BloodHound, and SysInternals) or we restrict none of them.\nAgain, this misrepresents Andrew\u0026rsquo;s argument. If you read his earlier article and his recent postings, there is a spectrum of a tool classes that could be considered for restriction. It\u0026rsquo;s not an all-or-nothing argument.\nAnecdotal Evidence Fallacy I haven\u0026rsquo;t seen attackers use publicly released OSTs; therefore, it\u0026rsquo;s not a problem.\nA personal anecdote does not provide a preponderance of evidence to refute an argument.\nTexas Sharpshooter Fallacy, APT XYZ, Government ABC, and FMA 123 do not use OST. Therefore, your argument that these tools are a problem is invalid.\nThis argument is cherry picking data and ignores the other adversaries that are using OST as part of their activities.\nPersonal Incredulity Fallacy I don\u0026rsquo;t understand the various aspects of OST release. It\u0026rsquo;s too complicated and can\u0026rsquo;t be correct.\nOne\u0026rsquo;s ability to understand an argument does not affect the validity of the claim.\n\u0026ldquo;No True Scotsman\u0026rdquo; Fallacy No real hacker would argue for the restriction of OST. Hackers break down systems - they don\u0026rsquo;t create them!\nNo true APT or FMA would use OSTs.\nThis is one of my favorite logical fallacies. It relies on universal generalizations to \u0026ldquo;inaccurately deflect counterexamples\u0026rdquo;.\nThe Problem of Restricting OST is too Difficult This argument supposes that Andrew is correct with regards to the problem of releasing OSTs in that attackers are using them, and it is negatively impacting the security community. They contend that setting up a system to control the release of OST is too difficult. \u0026ldquo;Pandora\u0026rsquo;s box has been opened.\u0026rdquo; Therefore, it is not worth pursuing solutions.\nWhile I do agree with these individuals that the problem is difficult, I disagree with their conclusion that it is not worth pursuing a solution. I argue that, in information security, our responsibility is to reduce the number of attacks and how costly those attacks are as much as possible. If the data shows that OST is being used on a high number of high-impact attacks, we thus have a responsibility to reduce that as much as possible.\n\u0026ldquo;Just because something is hard doesn\u0026rsquo;t mean it\u0026rsquo;s impossible.\u0026rdquo; - Lysa TerKeurst\nWe Have Already Solved the Problem I\u0026rsquo;ve seen this from a number of users replying to Andrew\u0026rsquo;s article. Their comments are something along the lines of \u0026ldquo;Imagine talking about the release of OST in 2019…\u0026rdquo; or \u0026ldquo;Are we really talking about this gain?\u0026quot;. I have two responses.\n I would consider this gatekeeping. You\u0026rsquo;re creating an old guard and saying \u0026ldquo;we have previously decided the answer. You, not a member of the old guard, have no right to question our previous decisions.\u0026rdquo; Instead of being excluding, be open. Explain your decisions and why those decisions were made. Invite people to the conversation. Things change. While your decisions may have been correct when they were made, we cannot conclude that every decision will remain true for eternity. We must continuously question our assumptions and pressure test our decisions to see if they continue to stand up. It may have been that economies have changed and more attackers are relying on OST than before, and thus this conversation is worth revisiting.  APTs and FMAs Aren\u0026rsquo;t Using OST The data speaks for itself. Very few organizations have access to the volume and quality of data that Andrew has. While I\u0026rsquo;m trying to avoid the \u0026ldquo;appeal to authority\u0026rdquo; fallacy, I do believe Andrew when he states APT33 is using Empire, Metasploit, and Mimikatz. I address the logical fallacy regarding \u0026ldquo;no true APT\u0026rdquo; earlier. I do not believe this argument holds weight, but I would appreciate a fresh perspective if you disagree.\nIf OST Wasn\u0026rsquo;t Released, FMAs Would Just Develop Their Own This is my favorite argument because I think it is the most valid. I\u0026rsquo;ll decompose it into the various sub-arguments. Let us hold that this argument is valid. From that, people who hold this position draw a number of conclusions.\n Since FMAs would just develop their own OSTs, I would rather they use publicly released ones so that I can develop signatures from tools I am aware of. Since FMAs would just develop their own OSTs, the harm I do in releasing a OST is minimal since the capability would exist anyways. (If you have another reason that I missed, please reach out and I\u0026rsquo;ll update the article.)  The first argument supposes that organizations as a whole will be able to improve their security. Don\u0026rsquo;t forget the anecdotal evidence fallacy. Just because your organization is equipped to quickly respond and detect new threats does not mean the majority (or even 25%) of organizations have those capabilities. I think one thing the security industry doesn’t yet universally understand is a VAST majority of companies can barely manage basic security compliance. I agree with GossiTheDog based on my years of working with a number of organizations as well as from anecdotes I\u0026rsquo;ve heard from peers in the industry. That does not mean I\u0026rsquo;m correct. I believe the correct approach would be for a survey of a large number of organizations across a variety of sizes and verticals to assess if they have the capability to respond to new tools and signatures as well as how quickly they deploy them. I foresee that very few will have this capability.\nThe second argument contends that these tools would exist anyways, so there is no harm in releasing them. The first argument is partially correct - FMAs would invest in developing OST if they were not publicly released. However, they would HAVE to invest in developing OST if these tools were not released. As Andrew states in his article, adversaries, just like blue teams, have finite resources. If they invest in developing these capabilities, it inherently means that they\u0026rsquo;re not investing in other areas. The relationship between information security and risk management is inextricable. Our goal, as security professionals, should be to make it as difficult as possible for adversaries to achieve their goals. We can do that by not only making our defenses better but by reducing their capabilities.\nRestricting the Release of OST Will Gatekeep the Offensive Security Community This group argues that releasing OST helps with inclusion for the offensive security community. They argue, from what I can tell, that restricting these tools creates artificial barriers where those with the tools arbitrarily decide who and who does not have access to these capabilities.\nWhat follows is probably my most controversial opinion. I agree with this argument, but I also do not believe that this is a bad thing. I believe that offensive security professionals do not currently exist. To quote Wikipedia, \u0026ldquo;Major milestones which may mark an occupation being identified as a profession include:\nan occupation becomes a full-time occupation the establishment of a training school the establishment of a university school the establishment of a local association the establishment of a national association of professional ethics the establishment of state licensing laws\u0026rdquo;\nWhile one could argue that some certifications (such as CISSP) can define an information security professional, offensive security does not currently meet these definitions. The bar to declare yourself an offensive security professional, start a company, and begin selling services is very low.\nWhen we look at some examples of professions, we may begin to notice a trend: medicine, accounting, law, architecture, etc. Mistakes are not tolerated in these professions. People could die or go to jail (or both). I believe that information security (to include offensive security) meets this same bar. Incorrect and misinformed judgements and decisions can result in disastrous effects. I am not proposing that \u0026ldquo;the haves\u0026rdquo; wall themselves off in an ivory castle from the \u0026ldquo;have nots\u0026rdquo;. I am saying that gatekeeping in and of itself is not bad especially when it is done to protect the quality of work, so consumers have confidence in the product and services they are procuring. An organization purchasing the services of an MSSP or Red Team should have confidence that the company and its employees have adequate experience and capabilities to provide the services they are describing. There also does not need to be just one gate. Just like in medicine there are a variety of paths, certifications, schools, and specialties, so too could a system exist in information security. Just as some medicines are over-the-counter and some are \u0026ldquo;gatekept\u0026rdquo; by prescribing physicians, so too could certain tools be restricted to those who have shown the technical and ethical capacity to responsible exercise them.\nIf you agree or disagree, I\u0026rsquo;d really appreciate thoughts and discussions on this point.\nYou Didn\u0026rsquo;t Address my Argument Please reach out and I\u0026rsquo;ll update my article accordingly.\nFinal Thoughts We can\u0026rsquo;t move towards a solution for a problem until we agree that there\u0026rsquo;s a problem. Andrew clearly has data that shows that tools such as PowerShell Empire, Responder, etc. are being used in real-world breaches and costing companies time and money. If you rush to conclude that that\u0026rsquo;s an acceptable cost, then I encourage you to reconsider - not that you\u0026rsquo;re wrong, but that you refuse to even have your beliefs challenge. We are engineers, scientists, operators, analysts, managers, leaders, and executives. But we are not zealots and no idea should beyond question.\nFinally, we are on the same side. Offensive security or defensive security - if you consider yourself a white hat, we have the same objective: making the world a more secure place. Please keep that in mind when discussing tough ideas with your peers. Mutural respect goes a long way.\n",
    "ref": "/blog/opensourceredteamtooling/"
  },{
    "title": "Getting Started with Praetorian’s ATT&CK™ Automation",
    "date": "",
    "description": "We've had a couple of people reach out about how to get started with our automation. This is part one of a multipart series where we'll cover how to get started with our automation. In this post, I show how to get the automation installed, a payload up and running, and executing a basic module.",
    "body": "Blog link: Getting Started with Praetorian\u0026rsquo;s ATT\u0026amp;CK Automation\n",
    "ref": "/blog/gettingstartedwithautomation/"
  },{
    "title": "Why Praetorian Benchmarks to MITRE ATT&CK™ and Why You Should Too",
    "date": "",
    "description": "I wrote a blog post on Praetorian's website explaining why we chose to benchmark detection and response to MITRE ATT&CK™.",
    "body": "Blog link: Why Praetorian Benchmarks to MITRE ATT\u0026amp;CK™ and Why You Should Too\n",
    "ref": "/blog/attackbenchmark/"
  },{
    "title": "Demonstrating the Future of Command and Control with Wikipedia",
    "date": "",
    "description": "This tool demonstrates what I believe will be the future of Command and Control (C2) for Red Teams and potentially Advanced Persistent Threats (APTs). With Domain Fronting slowly being killed by major CDNs and security teams getting better at looking for unusual sites (I still love that detection for new sites encrypted with LetsEncrypt certificates), Red Teams will look for alternate ways to hide their traffic. What better way than to utilize features of common sites that users are visiting anyways?",
    "body": "This work was inspired by my coworker\u0026rsquo;s (Josh Abraham) work to demonstrate alternative forms of Command and Control. Some of his POCs included demonstrations using Slack and ICMP for C2. While these techniques aren\u0026rsquo;t revolutionary, the simplicity of the POC and how hard it was to detect confirmed my suspicion that the future of C2 is going to be tunneling traffic via features of well-known applications.\nIn addition, there was a recent Twitter post from @randomuserid that got me thinking about what other sites besides Slack could be utilized. So, I browsed through the Alexa top sites and started seeing what features could be abused to facilitate C2 traffic. The first few were all linked in @randomuserid\u0026rsquo;s Google sheet, so I moved onto wikipedia.org. It didn\u0026rsquo;t take me long to stumble onto the Wikipedia API page.\nWhen determining what API to use, I had two goals: I wanted my traffic to be private (so no public pages) and I wanted to be able to send a not insignificant amount of data. I started with the options API as it seemed that setting options for the current user (perhaps steganography via a user profile picture?) would be a plausible solution. Browsing the list of user options, one immediately stood out: userjs-arbitraryKeyName. After a bit of experimentation, it proved trivial to set the value of this option. In addition, testing revealed that the maximum size was 65535 characters - more than enough for a useful C2 channel.\nAs an attacker, I would be hesitant to use this method as my primary C2 channel. A well-positioned defender could potentially notice the spike in traffic to Wikipedia. Instead, I would use this similar to the way that Red Teams utilize C2 over DNS - a backup channel in case the primary method is blocked.\nAs a defender, I would continue with standard methods for identifying unusual spikes in traffic. Play with the POC and see how much data has to be transferred to cause a spike in traffic to Wikipedia. If you\u0026rsquo;re doing HTTPS interception, it\u0026rsquo;s unlikely that standard users are using the Wikipedia API. All that being said, I believe this technique would be incredibly difficult to detect for your average Blue Team. I\u0026rsquo;m a firm believer in focusing security on the endpoint, and demonstrations like this continue to confirm that belief.\nProject link: https://github.com/dweezy-netsec/wikipedia-c2\n",
    "ref": "/blog/wikipediac2/"
  },{
    "title": "Open Source SaaS Reconnaissance Utilizing Subdomains",
    "date": "",
    "description": "Investigations into enumeration of an organization's SaaS tooling",
    "body": "On a recent Purple Team engagement, I was accessing the client’s Splunk cloud instance. Being my normal typo-filled self, I fat fingered the URL and went to clieent.splunkcloud.com instead of the correct client.splunkcloud.com. Instead of being redirected to the login portal, I received a DNS resolution error, fixed the typo, and moved on. A little while later, I was thinking about the typo again and wondered how this information leakage could be utilized by an attacker. To backup a little, a key part of an offensive campaign (and many say the most critical part) is recon. During this phase, the Red Team or malicious actor tries to obtain as much information about the target as possible. Key items of interest include organizational charts, employee lists, email formats, external address space, and technologies utilized by the organization. During this phase, the attacker wants to limit their interaction with the target as much as possible in order to avoid tipping off defenders. Most attackers avoid active scanning or interaction with the target during this phase.\nSo back to my story. I wanted to answer two questions: 1. Could this technique be applied to identify other organizations that are using Splunk Cloud? 2. What other SaaS applications could I enumerate in this way?\nTo answer the first question, I pulled a CSV of the Fortune 500 companies and ran a very rudimentary bash loop to curl sites based on the company names.\nwhile read p; do echo $p; curl $p.splunkcloud.com; done \u0026lt; ~/f500.txt \u0026gt; ~/f500_splunkcloud.txt\nSurprisingly, this returned 29 results. I then built a better wordlist for the first 150 companies and compared that with what was returned for the top 150 from the basic list. As an example, for “Capital One Financial”, my first list only contained “capitalonefinancial”. In the second list, I also included “capitalone”. This yieled 14 companies in the top 150 utilizing slunkcloud.com whereas the basic search had returned only 9 in the top 150. My basic search of the Fortune 500 yielded 29 companies using splunkcloud.com, though I’m sure that would return more results if I built a better wordlist.\nMy next goal was to see if this worked for other SaaS applications. In a few minutes of digging, I identified that Okta, Zoom, Atlassian, Slack, Box, and Zendesk also offer the same opportunities for open source recon (I’m intentionally avoiding the word “vulnerability” as I don’t believe this qualifies). A few of these sites were a bit more difficult to figure out. As an example, entering an invalid Okta url will still land you on a valid sign-in page. Fortunately, most organization upload their logo when making an Okta login page and we can grep for logoText and identify which of these pages are actually valid.\nwhile read p; do echo $p; curl -L $p.okta.com | grep logoText; done \u0026lt; ~/top150.txt \u0026gt; top150_okta.txt\nUtilizing my same top 150 list with some adjustments, I found that 31 of the top 150 Fortune 500 companies are using Okta. (Some false positives may be in there.)\nFor Slack, I simply had to look for the phrase “There’s been a glitch” which was returned on invalid subdomains. My list yielded quite a few false positives so I had to grep based on email addresses to ensure I had the right organization.\nwhile read p; do echo $p; curl -L $p.slack.com | tr -d \u0026lsquo;\\n\u0026rsquo; | grep -v \u0026ldquo;There\u0026rsquo;s been a glitch\u0026rdquo; ; done \u0026lt; ~/top150.txt \u0026gt; top150_slack.txt cat top150_slack.txt | grep -o \u0026lsquo;data-team-email-domains-formatted.{0,30}\u0026lsquo;\nI’d like to give a special shout-out to the folks at 1password.com. As far as I could tell, valid and invalid login pages presented the exact same content.\nSo what should you take away from this? As an attacker, build a list of SaaS applications to check prior to each engagement and add that to your playbook. As a defender, unless the vendor makes changes, I don’t see many options for preventing this type of recon. I’d be happy to receive emails if you have ideas. Make sure to monitor for logins to all your SaaS applications. Utilize a robust IAM solution. You could also consider setting up honey services. Say your a Microsoft Teams organization (some do exist, trust me), you could register for a basic Slack account and monitor for attempted logins. I haven’t done the research, but I’d bet that most failed logins would likely be a part of an advanced campaign as opposed to merely bots.\nIf you have experience in this area (either defending against this or using it on your Red Teams), I’d be happy to hear your stories!\nAll opinions in this article are my own.\nUpdate: I felt like this work needed a POC and I wanted to practice some basic Go, so I coded this up! I added a few more SaaS applications (SalesForce and Adobe Creative Cloud) and removed Zendesk as I felt there were too many false posities. Check it out https://github.com/daniel-infosec/subsaas.\n",
    "ref": "/blog/opensourcesubdomainrecon/"
  }]
