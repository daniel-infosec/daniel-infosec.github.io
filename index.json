[{
    "title": "Azure Flow Log Analysis",
    "date": "",
    "description": "Azure flow logs don't have the same instance ID that AWS flow logs do. So how do you figure out which VM the logs came from?",
    "body": "Intro Disclaimer I currently work at Snowflake and use the product on a daily basis for log analysis and threat detection. At the time of this writing, that probably adds bias to my article.\nAt Snowflake, we\u0026rsquo;re a multi-cloud environment. As part of the threat detection team, it\u0026rsquo;s my job to ensure that we\u0026rsquo;re monitoring. One of my tasks was to monitor our Azure flow logs for unusual behavior. Information on Azure Network Security Group (NSG) flow logs can be found at this link. When I started exploring this data source, I was frustrated to learn that, unlike the AWS VPC flow logs, there was no field that ties the flow log to the instance ID, or in the case of Azure, the Virtual Machine. The recommended solution from Microsoft was to use the IP address. However, the architecture I was working in had thousands of worker nodes created and destroyed frequently meaning that we would often see duplicate IP addresses within a short period of time due to NATing. If I couldn\u0026rsquo;t automatically tie the IP to a VM, the logs would be useless as our IR team needed that information in order to conduct automated response in case of a malicious event.\nPlotting Fortunately, I had SQL to help. To conduct my analysis, I used Snowflake, which is where we (obviously) store our logs. However, any SQL tool should offer the same capabilities below. In order to conduct this analysis, I used a trick taught to me by one of the data scientists at Snowflake. First, for Azure logs, IPs aren\u0026rsquo;t associated with VMs. They\u0026rsquo;re associated with NICs and a NIC is associated with a VM. Now, a lot of organizations use various tools and scripts that collect information about current assets periodically. However, considering Snowflake creates and destroys VMs and NICs so rapidly, this isn\u0026rsquo;t sufficient. We also needed to look at the Azure operation logs in order to establish confidence that we were looking at every NIC. Let\u0026rsquo;s take a look at some code.\nselect\rparse_json(propertiesresponseBody) as response_body,\rresponse_body:etag::string as etag,\rresponse_body:name::string as nic_name,\rresponse_body:location::string as nic_locatin,\rresponse_body:properties as nic_properties,\rresponse_body:tags as tags,\rresponse_body:type::string as nic_type,\rvalue:properties.privateIPAddress::string as nic_ip\rfrom\razure_operation_logs_v, lateral flatten(parse_json(properties:responseBody):properties.ipConfigurations)\rwhere\roperation_name = 'MICROSOFT.NETWORK/NETWORKINTERFACES/WRITE'\rYou can review the documentation on the structure here: Network Interfaces - Create Or Update.\nThis gets us information on NICs from our operation logs. We also had have scripts running that collect information on existing NICs. This is useful for looking at longer running VMs.\nwith nic_collect_logs as (\rselect\rid::string as nic_id,\retag::string as etag,\rnic_name::string as nic_name,\rnic_location::string as nic_location,\rnic_properties,\rtags,\rnic_type::string as nic_type,\rvalue:properties.privateIPAddress::string as nic_ip\r'collect_script' as source\rmin(recorded_at) as earliest,\rmax(recorded_at) as latest\rfrom\rAZURE_COLLECT_NETWORK_INTERFACES_V, lateral flatten(properties:ipConfigurations)\rgroup by\r1,2,3,4,5,6,7,8,9\r)\rselect * from nic_collect_logs;\rThis is slightly different. We wanted to remove duplicate NICs by using group by and also note the first time we saw the NIC and the most recent time we\u0026rsquo;ve seen the NIC. Let\u0026rsquo;s apply the same to the NICs collected by the operation logs.\nwith nic_create_op_logs_raw as (\rselect\rparse_json(propertiesresponseBody) as response_body,\rresponse_body:etag::string as etag,\rresponse_body:name::string as nic_name,\rresponse_body:location::string as nic_location,\rresponse_body:properties as nic_properties,\rresponse_body:tags as tags,\rresponse_body:type::string as nic_type,\rvalue:properties.privateIPAddress::string as nic_ip\rvalue:properties.entity::string as nic_id,\r'operation_logs' as source\rfrom\razure_operation_logs_v, lateral flatten(parse_json(properties:responseBody):properties.ipConfigurations)\rwhere\roperation_name = 'MICROSOFT.NETWORK/NETWORKINTERFACES/WRITE'\r),\rnic_create_op_logs as (\rselect\rnic_id,\retag,\rnic_name,\rnic_location,\rnic_properties,\rtags,\rnic_type,\rnic_ip,\rsource\rmin(event_time) as earliest,\rmax(event_time) as latest from nic_create_op_logs\rfrom nic_create_op_logs\rgroup by\r1,2,3,4,5,6,7,8,9\r)\rselect * from nic_create_op_logs;\rSo now we have NICs created by operational logs and NICs identified from our collection scripts. Let\u0026rsquo;s combine them together using union.\nwith all_nics as (\rselect * from nic_create_op_logs\rUNION\rselect * from nic_collect_logs\r)\rselect * from all_nics;\rNow, the issue comes up where we want to know the window of time that a NIC had a particular IP address. We can do this using the lead and lag functions to get the start and end window times for a NIC\u0026rsquo;s association with an IP address. In this case, we can use true_earliest as the window start. To find the window_end, we\u0026rsquo;ll use lead\nselect *,\rtrue_earliest as window_start,\rlead(true_earliest) over (\rpartition by nic_ip\rorder by true_earliest asc\r) as window_end\rfrom all_nics\r;\rThe Code Putting all the code together we have\nwith nic_collect_logs as (\rselect\rid::string as nic_id,\retag::string as etag,\rnic_name::string as nic_name,\rnic_location::string as nic_location,\rnic_properties,\rtags,\rnic_type::string as nic_type,\rvalue:properties.privateIPAddress::string as nic_ip\r'collect_script' as source\rmin(recorded_at) as earliest,\rmax(recorded_at) as latest\rfrom\rAZURE_COLLECT_NETWORK_INTERFACES_V, lateral flatten(properties:ipConfigurations)\rgroup by\r1,2,3,4,5,6,7,8,9\r),\rnic_create_op_logs_raw as (\rselect\rparse_json(propertiesresponseBody) as response_body,\rresponse_body:etag::string as etag,\rresponse_body:name::string as nic_name,\rresponse_body:location::string as nic_location,\rresponse_body:properties as nic_properties,\rresponse_body:tags as tags,\rresponse_body:type::string as nic_type,\rvalue:properties.privateIPAddress::string as nic_ip\rvalue:properties.entity::string as nic_id,\r'operation_logs' as source\rfrom\razure_operation_logs_v, lateral flatten(parse_json(properties:responseBody):properties.ipConfigurations)\rwhere\roperation_name = 'MICROSOFT.NETWORK/NETWORKINTERFACES/WRITE'\r),\rnic_create_op_logs as (\rselect\rnic_id,\retag,\rnic_name,\rnic_location,\rnic_properties,\rtags,\rnic_type,\rnic_ip,\rsource\rmin(event_time) as earliest,\rmax(event_time) as latest from nic_create_op_logs\rfrom nic_create_op_logs\rgroup by\r1,2,3,4,5,6,7,8,9\r),\rwith all_nics as (\rselect * from nic_create_op_logs\rUNION\rselect * from nic_collect_logs\r),\rselect *,\rtrue_earliest as window_start,\rlead(true_earliest) over (\rpartition by nic_ip\rorder by true_earliest asc\r) as window_end\rfrom all_nics;\rAnd now we have NICs and the IPs associated with them and the window of time when that IP was associated with that NIC.\nOur next step is to join this on VM data. That\u0026rsquo;s fairly easy.\nFor our Azure VMs, we did a similar exercise of joining data from Collect script and Operational logs. We\u0026rsquo;ll call this view AZURE_VMS_V. We\u0026rsquo;ll call our Azure NICs view AZURE_NICS_V.\nselect * from AZURE_NICS_V a\rjoin AZURE_VMS_V b on lower(a.id) = lower(b.properties['networkProfile']['networkInterfaces'][0]['id])\rA few notes - I used lower since there were some instances where the ID had varying cases and other instances where it didn\u0026rsquo;t. I\u0026rsquo;m attributing this to inconsistences with the APIs between collection scripts vs operational logs. You may also note that I used 0 instead of lateral flatten for accessing the network interfaces of the VMs. In our environment, we do not have VMs where there are multiple NICs. If you do, you can use lateral flatten again.\nThe final step is to join our flow logs on our NICs. In this way we\u0026rsquo;ll have joined Flow Log \u0026lt;\u0026gt; NIC \u0026lt;\u0026gt; VM. Fortunately, most of the hardwork is done.\nselect * from AZURE_FLOW_LOGS_V\rleft outer join AZURE_NICS_V on src_addr = nic_ip and event_time \u0026gt;= window_start and (event_time \u0026lt;= window_end or window_end is NULL)\rleft outer join AZURE_VMS_V on lower(AZURE_NICS_V.id) = lower(AZURE_VMS_V.properties['networkProfile']['networkInterfaces'][0]['id])\rFor joining the flow logs on the NIC, we want to ensure that the Flow Log event time occurred after the window start for the NIC and before the end time of the NIC. If the NIC still exists, the window_end would be NULL.\nIf you seek higher fidelity, you can also look at including tenant ID and subscription ID into your joins.\nThe obvious question is \u0026ldquo;how accurate is this?\u0026rdquo;\nLet\u0026rsquo;s check. I\u0026rsquo;ll call our above logic AZURE_FLOW_LOGS_JOIN_VMS_V. We\u0026rsquo;ll look for null VM IDs (where we couldn\u0026rsquo;t join on a VM) and the flow was recorded in the past day. (flow_recorded_at is a column we add during ingestion that records the time the flow was ingested into Snowflake.)\nselect count(*) from AZURE_FLOW_LOGS_JOIN_VMS_V\rwhere vm_id is NULL\rand flow_recorded_at \u0026gt;= dateadd(hour, -1, current_timestamp())\rand something similar for not null showed me that 99.75% of flow logs could be matched to their VM.\nConclusion This was a lot of work for something that AWS offers natively. I hope that as Azure matures their product that they\u0026rsquo;ll also offer similar levels of visibility.\nYou may be wondering why I used IPs instead of MAC addresses. When inspecting our logs, a vast number of NICs has IPs but no MAC addresses. I do not have a clear reason from Microsoft why that is at the time of this publication.\nI hope this work helps those working in security and Azure clouds. If you have other ideas or ways to improve the success rate, please let me know!\n",
    "ref": "/blog/azureflowloganalysis/"
  },{
    "title": "About",
    "date": "",
    "description": "Daniel's thoughts on infosec and trading",
    "body": "I\u0026rsquo;m a security engineer at Snowflake who also enjoys Muay Thai, BJJ, trading, and investing.\n",
    "ref": "/about/"
  },{
    "title": "Regarding SMS 2FA",
    "date": "",
    "description": "Responding to Tavis Ormandy's comments on SMS 2FA",
    "body": "I think an important question that Tavis either explicitly or accidentally omitted is \u0026ldquo;for whom\u0026rdquo;. I am not sure why he did not include this as it\u0026rsquo;s a critical component to his argument. If Tavis is stating that \u0026ldquo;SMS 2FA is ineffective for an enterprise\u0026rdquo;, then I would agree. The threat model that he is operating from is that an organization is being explicitly targeted by a motivated (though not necessarily extremely capable) attacker, who only needs minimal access to cause great harm. However, if Tavis is instead simply stating \u0026ldquo;SMS 2FA is ineffective\u0026rdquo; with no caveats, I would disagree.\nWhen considering SMS 2FA, one needs to look at their threat model and personal risk. To argue this point, I will attempt to address his points from the perspective of a technologically immature individual looking to secure their bank account. In this context, one\u0026rsquo;s threat model changes from \u0026ldquo;I need to protect all the accounts\u0026rdquo; to \u0026ldquo;I simply need to make sure my account is more secure than someone else\u0026rsquo;s\u0026rdquo;. From this perspective, SMS 2FA provides significantly more protection from credential stuffing than a username/password combination. In addition, the argument of \u0026ldquo;an attacker can simply move to a different service\u0026rdquo; no longer holds as not all accounts for this user are of equal importance. Access to their neopets account is significantly less valuable than their bank account.\nAnother one of Tavis' arguments is \u0026ldquo;Instead, why not simply randomly generate a good password for them, and instruct them to write it down or save it in their web browser? If they lose it, they can use your existing password reset procedure.\u0026rdquo; I think this argument ignores the reality of today\u0026rsquo;s world in that, if a commercial service forced that kind of policy, it would not be adopted by users. Anyone who has worked with a technologically immature user can attest to how common it is for users to forget or lose their password. While, again, I can see such a service working for an enterprise product, I do not see such an authentication scheme being acceptable for a B2C product.\nI\u0026rsquo;d also like to discuss his point of \u0026ldquo;We have a finite pool of good will with which we can advocate for the implementation of new security technologies. If we spend all that good will on irritating attackers, then by the time we’re ready to actually implement a solution, developers are not going to be interested.\u0026rdquo; As far as I can tell, Tavis is arguing that if one invests their time in implementing 2FA, then developers will not be interested in allowing or enforcing more secure options. Coming from the perspective of someone who has worked with numerous developers during my time with consulting, I do not feel that this is true. Security is top of mind for numerous companies. Account compromise not only has a cost to a user, but also has a cost the service provider. This can be in terms of refunds, time spent restoring access, or other direct resource cost. Additionally, I think Tavis is oversimplifying the argument. His statement of \u0026ldquo;all that good will\u0026rdquo; implies that implementing SMS 2FA will \u0026ldquo;use up\u0026rdquo; security\u0026rsquo;s ability to influence the security features a product will implement. He exhibits a false dichotomy logical fallacy where he represents the decision as either all or nothing.\nSecurity is ultimately a risk calculation. SMS 2FA doesn\u0026rsquo;t prevent malware and it doesn\u0026rsquo;t prevent phishing. However, risk is about using your resources to make the impact or likelihood of a failure event. SMS 2FA does just that by efficiently reducing the likelihood of credential stuffing attacks. Do I wish that all sites supported U2F for 2FA and do I encourage everyone to use a password manager to generate random passwords? Yes. Will I continue to recommend that enterprises not support SMS 2FA for SSO and other solutions? Yes. However, I recognize the reality that we live in that password-based authentication is a bandaid for the larger identity problem for the world wide web and B2C services. And I will continue to tell my friends and family that SMS 2FA is better than nothing. If you have counterarguments, please feel free to reach out via email or twitter, and I\u0026rsquo;d be happy to respond in an addendum to this post. Have a great weekend and remember to be respectful in your discourse.\n",
    "ref": "/blog/regardingsms2fa/"
  },{
    "title": "Adding to the Dialogue - On the Release of Offensive Security Tools (OST)",
    "date": "",
    "description": "After a lot of dialogue recently on the release of Offensive Security Tools, I thought I would add to the dialogue in a more long-form format.",
    "body": "Update 1 - I\u0026rsquo;m clarifying the definition of Advanced Persistent Threats (APTs) and Financially Motivated Actors (FMAs). I combined the two groups in the previous version. The content and focus of the discussion primarily centers on FMAs. APTs and FMAs can overlap in terms of TTPs, capabilities, personnel, countries, etc. What distinguishes them is motivation. FMAs, as their name implies, are financially motivated. APTs can have a number of motivations including financial, political, etc.\nTwitter is a complicated social networking platform. Then again, which platform isn\u0026rsquo;t? I\u0026rsquo;m fairly new to tweeting, but I\u0026rsquo;ve already found that it can be a tremendous resource when it comes to receiving up-to-date information on new techniques, upcoming talks, or nifty vulnerabilities. What I haven\u0026rsquo;t found it useful for is having discussions, especially those surrounding a controversial idea. This has manifested in my feed over the past month with the discussion around the release of Offensive Security Tools, primarily ignited by Andrew Thompson (@QW5kcmV3).\nhttps://twitter.com/ItsReallyNick/status/1171145995050205185/photo/1\nOffensive Security Tools Before you read anymore, please read Andrew\u0026rsquo;s blog post on the Unrestricted Release of Offensive Security Tools as I\u0026rsquo;ll be using verbiage defined in that article. (Based on some of the discussions on Twitter where people have straw manned Microsoft\u0026rsquo;s operating system as an Offensive Security Tool (OST), I have confidence that not many people have actually read the article.) Andrew\u0026rsquo;s primary argument is that the unrestricted release of OST is causing more harm than good to the world. He\u0026rsquo;s passionately championed this idea on Twitter, pointing to data such as that available from his employer (FireEye/Mandiant) about actual breaches. Reaction to his argument has ranged from agreement to vitriolic outrage. (I do concede that some of the negative feedback may be due to his tone, which has not always been the friendliest.) Opposition has accused him of gatekeeping, exaggerating the magnitude of the problem, or just being plain wrong. Others have argued that this is a solved problem and wonder why it\u0026rsquo;s even being discussed. Later in this article, I will attempt to address and refute some of those arguments.\nSafe Spaces The second article I\u0026rsquo;d like you to read is, on the surface, unrelated to the discussion at hand. It\u0026rsquo;s a recently published piece by James Hatch titled My semester with the snowflakes. In the article, James discusses some of the assumptions he made about the students of Yale University prior to his first semester at the institution and how these assumptions were summarily shattered. One of these assumptions regarded the term \u0026ldquo;safe space\u0026rdquo;. James had long regarded the term to mean a place to discuss ideas without having one\u0026rsquo;s feelings hurt. He learned the truth is quite the opposite. \u0026ldquo;What she [his fellow student meant by ‘safe space' was that she was happy to be in an environment where difficult subjects can be discussed openly, without the risk of disrespect or harsh judgement.\u0026rdquo; Throughout the rest of this post, I\u0026rsquo;d like to adopt the same definition.\nTwitter is Not a Safe Space At this point, I\u0026rsquo;m hoping you can see where I\u0026rsquo;m going with this section. The recent tone of the dialogue surrounding Andrew\u0026rsquo;s argument shows me that we are more likely moving further from a solution than towards one. Individuals on both sides of the argument have been quick to judge each other\u0026rsquo;s motives and credentials and have sometimes done so with a healthy dollop of disrespect. Twitter is not a \u0026ldquo;safe space\u0026rdquo;. Your most likely reaction to that sentence is \u0026ldquo;well, no shit\u0026rdquo;. Why, then, is so much of this discussion happening on Twitter? It clearly is not the place to discuss a topic that is clearly controversial. It is true that one can reach a global audience instantly; however, it encourages short, witty responses instead of a deep dialogue. Where then, do we turn? Conferences are another common vehicle for presenting ideas. Unfortunately, they too do not lend themselves to the idea of having a dialogue. Conferences are primarily unidirectional communication: a speaker lectures an audience. There may be time for questions, but its brevity does not encourage in depth dialogue. Additionally, conferences artificially restrict themselves to those with the resources (time, money, influence) to attend.\nAt this point in time, I am not aware of a currently existing solution for having such discussions. During my time in the Air Force, teams would sponsor \u0026ldquo;working groups\u0026rdquo; with attendees from a variety of squadrons with different perspectives to gather and discuss complicated and controversial ideas. Such a solution in the commercial space would require significant sponsorship and buy-in from employers who would lose valuable resources for a period of time as they worked on a problem affecting a community. But it\u0026rsquo;s an idea.\nGatekeeping Another term I want to define as I\u0026rsquo;ve seen it come up quite a bit is \u0026ldquo;gatekeeping\u0026rdquo;. I\u0026rsquo;ll go with the Urban Dictionary definition which is \u0026ldquo;when someone takes it upon themselves to decide who does or does not have access or rights to a community or identity.\u0026rdquo; I\u0026rsquo;ll address gatekeeping later in this article, but I wanted to clarify the definition I\u0026rsquo;ll be using.\nArguments Against the Restriction of OST Release At this point in time, I\u0026rsquo;ve seen a number of arguments against Andrew\u0026rsquo;s position. I\u0026rsquo;ll be addressing each of these in their own section.\n The problem of restricting OST is too difficult. We have already solved this problem. The problem is blown out of proportion. There are two components to this.  FMAs aren\u0026rsquo;t using public OSTs. If OST wasn\u0026rsquo;t released, FMAs would develop their own.   Restricting the release of OST will gatekeep the offensive security community.  Logical Fallacies In addressing Andrew\u0026rsquo;s arguments, I\u0026rsquo;ve seen quite a few logical fallacies. I\u0026rsquo;ll address some examples I\u0026rsquo;ve seen here, so that others can be aware. For those who have used these arguments, I would encourage you to review the list and think to yourself if you\u0026rsquo;re committing one of these fallacies when engaging in debates.\nStraw Man Fallacy Windows is used by attackers - therefore you\u0026rsquo;re proposing we restrict the release of Operating Systems. This is obviously ludicrous, therefore, the argument to restrict OST is invalid.\nHere, someone is misrepresenting Andrew\u0026rsquo;s argument by providing an easy to refute example. It is \u0026ldquo;superficially similar but ultimately not equal version of [Andrew\u0026rsquo;s] real stance\u0026rdquo;.\nBandwagon Fallacy The majority of the community thinks restricting OST is wrong. Therefore, your proposition is invalid.\nJust because the majority agrees on something, doesn\u0026rsquo;t mean it\u0026rsquo;s correct.\nThe False Dilemma Fallacy Either we restrict all tools that can be used for attackers (including Empire, BloodHound, and SysInternals) or we restrict none of them.\nAgain, this misrepresents Andrew\u0026rsquo;s argument. If you read his earlier article and his recent postings, there is a spectrum of a tool classes that could be considered for restriction. It\u0026rsquo;s not an all-or-nothing argument.\nAnecdotal Evidence Fallacy I haven\u0026rsquo;t seen attackers use publicly released OSTs; therefore, it\u0026rsquo;s not a problem.\nA personal anecdote does not provide a preponderance of evidence to refute an argument.\nTexas Sharpshooter Fallacy, APT XYZ, Government ABC, and FMA 123 do not use OST. Therefore, your argument that these tools are a problem is invalid.\nThis argument is cherry picking data and ignores the other adversaries that are using OST as part of their activities.\nPersonal Incredulity Fallacy I don\u0026rsquo;t understand the various aspects of OST release. It\u0026rsquo;s too complicated and can\u0026rsquo;t be correct.\nOne\u0026rsquo;s ability to understand an argument does not affect the validity of the claim.\n\u0026ldquo;No True Scotsman\u0026rdquo; Fallacy No real hacker would argue for the restriction of OST. Hackers break down systems - they don\u0026rsquo;t create them!\nNo true APT or FMA would use OSTs.\nThis is one of my favorite logical fallacies. It relies on universal generalizations to \u0026ldquo;inaccurately deflect counterexamples\u0026rdquo;.\nThe Problem of Restricting OST is too Difficult This argument supposes that Andrew is correct with regards to the problem of releasing OSTs in that attackers are using them, and it is negatively impacting the security community. They contend that setting up a system to control the release of OST is too difficult. \u0026ldquo;Pandora\u0026rsquo;s box has been opened.\u0026rdquo; Therefore, it is not worth pursuing solutions.\nWhile I do agree with these individuals that the problem is difficult, I disagree with their conclusion that it is not worth pursuing a solution. I argue that, in information security, our responsibility is to reduce the number of attacks and how costly those attacks are as much as possible. If the data shows that OST is being used on a high number of high-impact attacks, we thus have a responsibility to reduce that as much as possible.\n\u0026ldquo;Just because something is hard doesn\u0026rsquo;t mean it\u0026rsquo;s impossible.\u0026rdquo; - Lysa TerKeurst\nWe Have Already Solved the Problem I\u0026rsquo;ve seen this from a number of users replying to Andrew\u0026rsquo;s article. Their comments are something along the lines of \u0026ldquo;Imagine talking about the release of OST in 2019…\u0026rdquo; or \u0026ldquo;Are we really talking about this gain?\u0026rdquo;. I have two responses.\n I would consider this gatekeeping. You\u0026rsquo;re creating an old guard and saying \u0026ldquo;we have previously decided the answer. You, not a member of the old guard, have no right to question our previous decisions.\u0026rdquo; Instead of being excluding, be open. Explain your decisions and why those decisions were made. Invite people to the conversation. Things change. While your decisions may have been correct when they were made, we cannot conclude that every decision will remain true for eternity. We must continuously question our assumptions and pressure test our decisions to see if they continue to stand up. It may have been that economies have changed and more attackers are relying on OST than before, and thus this conversation is worth revisiting.  APTs and FMAs Aren\u0026rsquo;t Using OST The data speaks for itself. Very few organizations have access to the volume and quality of data that Andrew has. While I\u0026rsquo;m trying to avoid the \u0026ldquo;appeal to authority\u0026rdquo; fallacy, I do believe Andrew when he states APT33 is using Empire, Metasploit, and Mimikatz. I address the logical fallacy regarding \u0026ldquo;no true APT\u0026rdquo; earlier. I do not believe this argument holds weight, but I would appreciate a fresh perspective if you disagree.\nIf OST Wasn\u0026rsquo;t Released, FMAs Would Just Develop Their Own This is my favorite argument because I think it is the most valid. I\u0026rsquo;ll decompose it into the various sub-arguments. Let us hold that this argument is valid. From that, people who hold this position draw a number of conclusions.\n Since FMAs would just develop their own OSTs, I would rather they use publicly released ones so that I can develop signatures from tools I am aware of. Since FMAs would just develop their own OSTs, the harm I do in releasing a OST is minimal since the capability would exist anyways. (If you have another reason that I missed, please reach out and I\u0026rsquo;ll update the article.)  The first argument supposes that organizations as a whole will be able to improve their security. Don\u0026rsquo;t forget the anecdotal evidence fallacy. Just because your organization is equipped to quickly respond and detect new threats does not mean the majority (or even 25%) of organizations have those capabilities. I think one thing the security industry doesn’t yet universally understand is a VAST majority of companies can barely manage basic security compliance. I agree with GossiTheDog based on my years of working with a number of organizations as well as from anecdotes I\u0026rsquo;ve heard from peers in the industry. That does not mean I\u0026rsquo;m correct. I believe the correct approach would be for a survey of a large number of organizations across a variety of sizes and verticals to assess if they have the capability to respond to new tools and signatures as well as how quickly they deploy them. I foresee that very few will have this capability.\nThe second argument contends that these tools would exist anyways, so there is no harm in releasing them. The first argument is partially correct - FMAs would invest in developing OST if they were not publicly released. However, they would HAVE to invest in developing OST if these tools were not released. As Andrew states in his article, adversaries, just like blue teams, have finite resources. If they invest in developing these capabilities, it inherently means that they\u0026rsquo;re not investing in other areas. The relationship between information security and risk management is inextricable. Our goal, as security professionals, should be to make it as difficult as possible for adversaries to achieve their goals. We can do that by not only making our defenses better but by reducing their capabilities.\nRestricting the Release of OST Will Gatekeep the Offensive Security Community This group argues that releasing OST helps with inclusion for the offensive security community. They argue, from what I can tell, that restricting these tools creates artificial barriers where those with the tools arbitrarily decide who and who does not have access to these capabilities.\nWhat follows is probably my most controversial opinion. I agree with this argument, but I also do not believe that this is a bad thing. I believe that offensive security professionals do not currently exist. To quote Wikipedia, \u0026ldquo;Major milestones which may mark an occupation being identified as a profession include:\nan occupation becomes a full-time occupation the establishment of a training school the establishment of a university school the establishment of a local association the establishment of a national association of professional ethics the establishment of state licensing laws\u0026rdquo;\nWhile one could argue that some certifications (such as CISSP) can define an information security professional, offensive security does not currently meet these definitions. The bar to declare yourself an offensive security professional, start a company, and begin selling services is very low.\nWhen we look at some examples of professions, we may begin to notice a trend: medicine, accounting, law, architecture, etc. Mistakes are not tolerated in these professions. People could die or go to jail (or both). I believe that information security (to include offensive security) meets this same bar. Incorrect and misinformed judgements and decisions can result in disastrous effects. I am not proposing that \u0026ldquo;the haves\u0026rdquo; wall themselves off in an ivory castle from the \u0026ldquo;have nots\u0026rdquo;. I am saying that gatekeeping in and of itself is not bad especially when it is done to protect the quality of work, so consumers have confidence in the product and services they are procuring. An organization purchasing the services of an MSSP or Red Team should have confidence that the company and its employees have adequate experience and capabilities to provide the services they are describing. There also does not need to be just one gate. Just like in medicine there are a variety of paths, certifications, schools, and specialties, so too could a system exist in information security. Just as some medicines are over-the-counter and some are \u0026ldquo;gatekept\u0026rdquo; by prescribing physicians, so too could certain tools be restricted to those who have shown the technical and ethical capacity to responsible exercise them.\nIf you agree or disagree, I\u0026rsquo;d really appreciate thoughts and discussions on this point.\nYou Didn\u0026rsquo;t Address my Argument Please reach out and I\u0026rsquo;ll update my article accordingly.\nFinal Thoughts We can\u0026rsquo;t move towards a solution for a problem until we agree that there\u0026rsquo;s a problem. Andrew clearly has data that shows that tools such as PowerShell Empire, Responder, etc. are being used in real-world breaches and costing companies time and money. If you rush to conclude that that\u0026rsquo;s an acceptable cost, then I encourage you to reconsider - not that you\u0026rsquo;re wrong, but that you refuse to even have your beliefs challenge. We are engineers, scientists, operators, analysts, managers, leaders, and executives. But we are not zealots and no idea should beyond question.\nFinally, we are on the same side. Offensive security or defensive security - if you consider yourself a white hat, we have the same objective: making the world a more secure place. Please keep that in mind when discussing tough ideas with your peers. Mutural respect goes a long way.\n",
    "ref": "/blog/opensourceredteamtooling/"
  },{
    "title": "Getting Started with Praetorian’s ATT&CK™ Automation",
    "date": "",
    "description": "We've had a couple of people reach out about how to get started with our automation. This is part one of a multipart series where we'll cover how to get started with our automation. In this post, I show how to get the automation installed, a payload up and running, and executing a basic module.",
    "body": "Blog link: Getting Started with Praetorian\u0026rsquo;s ATT\u0026amp;CK Automation\n",
    "ref": "/blog/gettingstartedwithautomation/"
  },{
    "title": "Why Praetorian Benchmarks to MITRE ATT&CK™ and Why You Should Too",
    "date": "",
    "description": "I wrote a blog post on Praetorian's website explaining why we chose to benchmark detection and response to MITRE ATT&CK™.",
    "body": "Blog link: Why Praetorian Benchmarks to MITRE ATT\u0026amp;CK™ and Why You Should Too\n",
    "ref": "/blog/attackbenchmark/"
  },{
    "title": "Demonstrating the Future of Command and Control with Wikipedia",
    "date": "",
    "description": "This tool demonstrates what I believe will be the future of Command and Control (C2) for Red Teams and potentially Advanced Persistent Threats (APTs). With Domain Fronting slowly being killed by major CDNs and security teams getting better at looking for unusual sites (I still love that detection for new sites encrypted with LetsEncrypt certificates), Red Teams will look for alternate ways to hide their traffic. What better way than to utilize features of common sites that users are visiting anyways?",
    "body": "This work was inspired by my coworker\u0026rsquo;s (Josh Abraham) work to demonstrate alternative forms of Command and Control. Some of his POCs included demonstrations using Slack and ICMP for C2. While these techniques aren\u0026rsquo;t revolutionary, the simplicity of the POC and how hard it was to detect confirmed my suspicion that the future of C2 is going to be tunneling traffic via features of well-known applications.\nIn addition, there was a recent Twitter post from @randomuserid that got me thinking about what other sites besides Slack could be utilized. So, I browsed through the Alexa top sites and started seeing what features could be abused to facilitate C2 traffic. The first few were all linked in @randomuserid\u0026rsquo;s Google sheet, so I moved onto wikipedia.org. It didn\u0026rsquo;t take me long to stumble onto the Wikipedia API page.\nWhen determining what API to use, I had two goals: I wanted my traffic to be private (so no public pages) and I wanted to be able to send a not insignificant amount of data. I started with the options API as it seemed that setting options for the current user (perhaps steganography via a user profile picture?) would be a plausible solution. Browsing the list of user options, one immediately stood out: userjs-arbitraryKeyName. After a bit of experimentation, it proved trivial to set the value of this option. In addition, testing revealed that the maximum size was 65535 characters - more than enough for a useful C2 channel.\nAs an attacker, I would be hesitant to use this method as my primary C2 channel. A well-positioned defender could potentially notice the spike in traffic to Wikipedia. Instead, I would use this similar to the way that Red Teams utilize C2 over DNS - a backup channel in case the primary method is blocked.\nAs a defender, I would continue with standard methods for identifying unusual spikes in traffic. Play with the POC and see how much data has to be transferred to cause a spike in traffic to Wikipedia. If you\u0026rsquo;re doing HTTPS interception, it\u0026rsquo;s unlikely that standard users are using the Wikipedia API. All that being said, I believe this technique would be incredibly difficult to detect for your average Blue Team. I\u0026rsquo;m a firm believer in focusing security on the endpoint, and demonstrations like this continue to confirm that belief.\nProject link: https://github.com/dweezy-netsec/wikipedia-c2\n",
    "ref": "/blog/wikipediac2/"
  },{
    "title": "Open Source SaaS Reconnaissance Utilizing Subdomains",
    "date": "",
    "description": "Investigations into enumeration of an organization's SaaS tooling",
    "body": "On a recent Purple Team engagement, I was accessing the client’s Splunk cloud instance. Being my normal typo-filled self, I fat fingered the URL and went to clieent.splunkcloud.com instead of the correct client.splunkcloud.com. Instead of being redirected to the login portal, I received a DNS resolution error, fixed the typo, and moved on. A little while later, I was thinking about the typo again and wondered how this information leakage could be utilized by an attacker. To backup a little, a key part of an offensive campaign (and many say the most critical part) is recon. During this phase, the Red Team or malicious actor tries to obtain as much information about the target as possible. Key items of interest include organizational charts, employee lists, email formats, external address space, and technologies utilized by the organization. During this phase, the attacker wants to limit their interaction with the target as much as possible in order to avoid tipping off defenders. Most attackers avoid active scanning or interaction with the target during this phase.\nSo back to my story. I wanted to answer two questions: 1. Could this technique be applied to identify other organizations that are using Splunk Cloud? 2. What other SaaS applications could I enumerate in this way?\nTo answer the first question, I pulled a CSV of the Fortune 500 companies and ran a very rudimentary bash loop to curl sites based on the company names.\nwhile read p; do echo $p; curl $p.splunkcloud.com; done \u0026lt; ~/f500.txt \u0026gt; ~/f500_splunkcloud.txt\nSurprisingly, this returned 29 results. I then built a better wordlist for the first 150 companies and compared that with what was returned for the top 150 from the basic list. As an example, for “Capital One Financial”, my first list only contained “capitalonefinancial”. In the second list, I also included “capitalone”. This yieled 14 companies in the top 150 utilizing slunkcloud.com whereas the basic search had returned only 9 in the top 150. My basic search of the Fortune 500 yielded 29 companies using splunkcloud.com, though I’m sure that would return more results if I built a better wordlist.\nMy next goal was to see if this worked for other SaaS applications. In a few minutes of digging, I identified that Okta, Zoom, Atlassian, Slack, Box, and Zendesk also offer the same opportunities for open source recon (I’m intentionally avoiding the word “vulnerability” as I don’t believe this qualifies). A few of these sites were a bit more difficult to figure out. As an example, entering an invalid Okta url will still land you on a valid sign-in page. Fortunately, most organization upload their logo when making an Okta login page and we can grep for logoText and identify which of these pages are actually valid.\nwhile read p; do echo $p; curl -L $p.okta.com | grep logoText; done \u0026lt; ~/top150.txt \u0026gt; top150_okta.txt\nUtilizing my same top 150 list with some adjustments, I found that 31 of the top 150 Fortune 500 companies are using Okta. (Some false positives may be in there.)\nFor Slack, I simply had to look for the phrase “There’s been a glitch” which was returned on invalid subdomains. My list yielded quite a few false positives so I had to grep based on email addresses to ensure I had the right organization.\nwhile read p; do echo $p; curl -L $p.slack.com | tr -d \u0026lsquo;\\n\u0026rsquo; | grep -v \u0026ldquo;There\u0026rsquo;s been a glitch\u0026rdquo; ; done \u0026lt; ~/top150.txt \u0026gt; top150_slack.txt cat top150_slack.txt | grep -o \u0026lsquo;data-team-email-domains-formatted.{0,30}'\nI’d like to give a special shout-out to the folks at 1password.com. As far as I could tell, valid and invalid login pages presented the exact same content.\nSo what should you take away from this? As an attacker, build a list of SaaS applications to check prior to each engagement and add that to your playbook. As a defender, unless the vendor makes changes, I don’t see many options for preventing this type of recon. I’d be happy to receive emails if you have ideas. Make sure to monitor for logins to all your SaaS applications. Utilize a robust IAM solution. You could also consider setting up honey services. Say your a Microsoft Teams organization (some do exist, trust me), you could register for a basic Slack account and monitor for attempted logins. I haven’t done the research, but I’d bet that most failed logins would likely be a part of an advanced campaign as opposed to merely bots.\nIf you have experience in this area (either defending against this or using it on your Red Teams), I’d be happy to hear your stories!\nAll opinions in this article are my own.\nUpdate: I felt like this work needed a POC and I wanted to practice some basic Go, so I coded this up! I added a few more SaaS applications (SalesForce and Adobe Creative Cloud) and removed Zendesk as I felt there were too many false posities. Check it out https://github.com/daniel-infosec/subsaas.\n",
    "ref": "/blog/opensourcesubdomainrecon/"
  }]
